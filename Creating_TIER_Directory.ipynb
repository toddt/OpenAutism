{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates new directory with con image, t-maps, z-maps, and temporal BOLD 4D images\n",
    "# and design files for OpenAutism (per subject, per task, per run)\n",
    "#only copy over files on the spreadsheet (/om/user/rezzo/EXPERIMENT_INFO/tomloc_subject_info_internal.csv)\n",
    "\n",
    "# FOR FUTURE / PENDING:\n",
    "# once second level is run, must import one con, t-map and z-map per subject (now importing \n",
    "# one per run)\n",
    "\n",
    "# some subjects are in more than one experiment (e.g. SAX_OA_## in both exp1\n",
    "# and exp2). In these cases, all runs are copied over. There is a second option, by using the function\n",
    "# identify_greater_motion, which returns a list of the old subjectids to be eliminated. Based framewise displacement. \n",
    "\n",
    "pilot = 1        # 1 = pilot_data / 0 = analysis_data\n",
    "\n",
    "# import modules\n",
    "from glob import glob\n",
    "import re\n",
    "import os.path\n",
    "from itertools import repeat\n",
    "import csv\n",
    "from shutil import copyfile\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Obtain_OA_list(tmp_root):\n",
    "    all_OA = []\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \",\"):\n",
    "            all_OA.append(line[0])\n",
    "    return all_OA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional function to use, if decide to remove the experiment of duplicate subject based \n",
    "# on greater motion.\n",
    "\n",
    "def identify_greater_motion(subject1, subject2):\n",
    "    tmp_root = '/om/user/rezzo/INCLUDE_final.csv'\n",
    "\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        mot1 = []\n",
    "        mot2 = []\n",
    "        subjectarray = [subject1, subject2]\n",
    "        readCSV = csv.reader(tsv, delimiter='\\t')\n",
    "        for line in readCSV:\n",
    "            if subject1 == line[2]:\n",
    "                mot1.append(float(line[9]))\n",
    "                np.array(mot1)\n",
    "            if subject2 == line[2]:\n",
    "                mot2.append(float(line[9]))\n",
    "                np.array(mot2)\n",
    "    if np.mean(mot1) > np.mean(mot2):\n",
    "        return subject1\n",
    "    elif np.mean(mot2) > np.mean(mot1):\n",
    "        return subject2\n",
    "    else:\n",
    "        return random.choice(subjectarray) # for temporarily missing data, just choose randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_2int(runs):\n",
    "    new_runs = [x.strip() for x in runs.split(', ')]\n",
    "    counter = -1\n",
    "    for el in new_runs:\n",
    "        counter = counter+1\n",
    "        new_runs[counter] = int(el)\n",
    "    return new_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sub(name):\n",
    "    if name[:4] == 'sub-':\n",
    "        name = name[4:]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looks for subjects that participated in > 1 experiment\n",
    "\n",
    "conversion = '/om/user/rezzo/Subject_Conversion_Table.csv'\n",
    "df = pd.read_csv(conversion, header=None)\n",
    "\n",
    "# make list of all duplicate items\n",
    "a = df[0].tolist()\n",
    "dup_list = [k for k,v in Counter(a).items() if v>1]\n",
    "\n",
    "repeat_subjects = []\n",
    "remove_subjects = []\n",
    "\n",
    "for el in dup_list:\n",
    "    with open(conversion, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \",\"):\n",
    "            if el == line[0]:\n",
    "                repeat_subjects.append(line[1])\n",
    "\n",
    "list_of_duplicates = [repeat_subjects[i:i + 2] for i in range(0, len(repeat_subjects), 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject look up table conversion (IGNORING undescores)\n",
    "def Convert_Subname(Oldname):\n",
    "    tmp_root = '/om/user/rezzo/Subject_Conversion_Table.csv'\n",
    "\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \",\"):\n",
    "            if Oldname == line[1].replace(\"_\",\"\"):\n",
    "                Newname = line[0]\n",
    "            else:\n",
    "                continue\n",
    "    return Newname  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Underscores(list_of_names):\n",
    "    counter = 0\n",
    "    for el in list_of_names:\n",
    "        list_of_names[counter] = el.replace(\"_\",\"\")\n",
    "        counter = counter + 1\n",
    "    return list_of_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/om/group/saxelab/OpenAutism/Analysis/first_level_standard.py/'\n",
    "\n",
    "if pilot == 1:\n",
    "    OA_dir = '/om/user/rezzo/OpenAutism/pilot_data/SUBJECTS/'\n",
    "    conversion = '/om/user/rezzo/EXPERIMENT_INFO/tomloc_subject_info_internal.csv'\n",
    "else:\n",
    "    OA_dir = '/om/user/rezzo/OpenAutism/analysis_data/SUBJECTS/'\n",
    "    conversion = [] # need this file\n",
    "\n",
    "df = pd.read_csv(conversion)\n",
    "df\n",
    "dup_list = []\n",
    "UNIQUE_OA_IDS = df['OAID'].unique()\n",
    "UNIQUE_TASKS = df['func_task'].unique()\n",
    "\n",
    "# for each unique OA subject, return row\n",
    "for newname in UNIQUE_OA_IDS:\n",
    "    \n",
    "    if pilot == 1:\n",
    "        UNIQUE_OA_IDS = [ task for task in UNIQUE_OA_IDS if \"tomloc\" in task ]\n",
    "    else:\n",
    "        UNIQUE_OA_IDS = [ task for task in UNIQUE_OA_IDS if \"tomloc\" not in task ]\n",
    "\n",
    "        #create OA directory in TIER folder is non-existent\n",
    "    if (not os.path.exists(OA_dir+newname)):\n",
    "\n",
    "        proc_streams = ['first_level_standard.py','first_level_aroma.py','first_level_gorgolewski.py']\n",
    "        time_streams = ['first_level_standard_timeseries.py','first_level_aroma_timeseries.py','first_level_gorgolewski_timeseries.py']\n",
    "\n",
    "        for stream in range(0, len(proc_streams)):\n",
    "\n",
    "            stream_abrv = find_between(proc_streams[stream], \"level_\", \".py\" )\n",
    "\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'first_level_analyses','BOLD_data')) \n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','multivariate'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','lateralization'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','magnitude'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','interregional_cor'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','temporal_variance'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','MISC','individual_roi_masks'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','MISC','mean_roi_Temporal_Signal'))\n",
    "\n",
    "            for task in UNIQUE_TASKS:\n",
    "                # create unique rows\n",
    "                dup_list = []\n",
    "                dup_list.append(df.index[(df['OAID'] == newname) & (df['func_task'] == task)].tolist())\n",
    "                task = Remove_Underscores([task])\n",
    "                task = task[0]\n",
    "                run_counter = 0\n",
    "                \n",
    "                for items in dup_list: # each item is the indices list of all runs for a task (across experiments)\n",
    "                    site = df.iloc[items]['data_set'].astype(str)\n",
    "                    site = site.tolist() #site\n",
    "                    found = df.iloc[items]['SUBJID'].astype(str)\n",
    "                    found = Remove_Underscores(found.tolist()) #subjid\n",
    "                    \n",
    "                    for el in range(0,len(items)): \n",
    "                        items_index = items[el]\n",
    "                        runs = df.iloc[items_index]['BIDS_runs']\n",
    "                        runs = Convert_2int(runs)\n",
    "                        \n",
    "                        for run in runs:\n",
    "                            run_counter = run_counter+1\n",
    "                            src_dir = '/om/group/saxelab/OpenAutism/Analysis/'+proc_streams[stream]+'/'+site[el]+'/sub-'+found[el]+'/'+task+'/model/run'+str(run)+'/'\n",
    "                            design_dir = '/om/group/saxelab/OpenAutism/data/TextFiles/tsv_text_files/'+site[el]+'/'\n",
    "                            raw_dir = \"/om/group/saxelab/OpenAutism/Analysis/\"+time_streams[stream]+'/'+site[el]+\"/sub-\"+found[el]+'/'+task+\"/model/run\"+str(run)+'/'\n",
    "                            \n",
    "                            try:\n",
    "                                #con, tmap, zmap\n",
    "                                prefixed1 = [filename for filename in sorted(os.listdir(src_dir)) if (filename.startswith(\"con_1_\") and filename.endswith(\"_tstat.nii.gz\"))]\n",
    "                                copyfile(src_dir+prefixed1[0], OA_dir+newname+'/'+stream_abrv+'/first_level_analyses/tstat1_'+task+'_'+str(run_counter)+'.nii.gz')\n",
    "                                prefixed2 = [filename for filename in sorted(os.listdir(src_dir)) if (filename.startswith(\"con_1_\") and filename.endswith(\"_cope.nii.gz\"))]\n",
    "                                copyfile(src_dir+prefixed2[0], OA_dir+newname+'/'+stream_abrv+'/first_level_analyses/cope1_'+task+'_'+str(run_counter)+'.nii.gz')\n",
    "                                prefixed3 = [filename for filename in sorted(os.listdir(src_dir)) if (filename.startswith(\"con_1_\") and filename.endswith(\"_zstat.nii.gz\"))]\n",
    "                                copyfile(src_dir+prefixed3[0], OA_dir+newname+'/'+stream_abrv+'/first_level_analyses/zstat1_'+task+'_'+str(run_counter)+'.nii.gz')\n",
    "                            except FileNotFoundError:\n",
    "                                print('WARNING: missing image file(s) in '+src_dir)\n",
    "                                \n",
    "                            try:\n",
    "                                #temporal runs\n",
    "                                prefixed4 = [filename for filename in sorted(os.listdir(raw_dir)) if (filename.startswith(\"res4d\") and filename.endswith(\".nii.gz\"))]\n",
    "                                copyfile(raw_dir+prefixed4[0], OA_dir+newname+'/'+stream_abrv+'/first_level_analyses/BOLD_data/temporaldata_'+task+'_'+str(run_counter)+'.nii.gz')\n",
    "                            except FileNotFoundError:\n",
    "                                print('WARNING: missing temopral file(s) in '+raw_dir)\n",
    "                                \n",
    "                            try:\n",
    "                                #design file\n",
    "                                copyfile(design_dir+'sub-'+found[el]+'_task-'+task+'_run-00'+str(run)+'_events.tsv', OA_dir+newname+'/'+stream_abrv+'/first_level_analyses/BOLD_data/design_'+task+'_'+'run-00'+str(run_counter)+'.tsv') \n",
    "                            except FileNotFoundError:\n",
    "                                print('WARNING: missing design file(s) in '+design_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
