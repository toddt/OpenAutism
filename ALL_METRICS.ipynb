{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERAL ROI INFORMATION:\n",
    "# This script uses functional roi niftis (converted from.img files and scaled in FSL to correct\n",
    "# size in each dimension). These parcels are used as binary masks on output from first-level \n",
    "# analysis. \n",
    "# This yields a hypothesis space for each roi (same location for every \n",
    "# subject). These ROI's are face parcels from the Kanwisher website http://web.mit.edu/bcs/nklab/GSS.shtml\n",
    "# with the exception of DMFPC and AMY (taken from saxelab).\n",
    "\n",
    "## MAGNITUDE\n",
    "# The top N (default = 50) voxels are then selected from these hypothesis space based on highest t-statistic\n",
    "# (no threshold, and no constraint on contiguity). This defines the ROIs per subject.\n",
    "# The corresponding con values are averaged as the mean magnitude summary statistic.\n",
    "# Also computes average position\n",
    "\n",
    "## LATERALIZATION\n",
    "# Different MASKS used. Masks created by summing hypothesis space and flipped opposite space, \n",
    "# e.g. rTPJ computed as voxels within rTPJ parcel and flipped lTPJ. See commands:\n",
    "# TO FLIP: fslswapdim data -x y z flip_data\n",
    "# TO ADD: fslmaths mask1 -add mask2 -bin output ... (e.g. mask1 = rTPJ original, mask 2 = lTPJ flipped)\n",
    "# Count NUMBER of voxels above a certain p value (0.01 or 0.001) to do calculations: L - R / L + R\n",
    "\n",
    "## INTERREGIONAL CORRELATIONS\n",
    "# (fill in)\n",
    "\n",
    "## TEMPORAL NOISE\n",
    "# (fill in)\n",
    "\n",
    "## MULTIVARIATE VECTORS\n",
    "# This is computed over entire hypothesis space, as a vector of all contrast values within the parcel.\n",
    "\n",
    "# SAVES: \n",
    "# roi_individual_masks & roi_contrast_values (top N voxels per subject), \n",
    "# and text files per metric computations that can be opened as a dataframe.\n",
    "\n",
    "# MASKS DIRECTORY\n",
    "# /om/user/rezzo/OpenAutism/analysis_data/MASKS/STANDARD_MASKS\n",
    "# /om/user/rezzo/OpenAutism/analysis_data/MASKS/LATERAL_MASKS\n",
    "\n",
    "# Note : to convert Analyze format to Nifti format:\n",
    "# load in header, image (and .mat) files into the folder\n",
    "# fslchfiletype NIFTI_GZ RTPJ_xyz.nii.gz RTPJ_xyz\n",
    "\n",
    "# SETTINGS\n",
    "pilot = 1           # pilot == 1 runs on tomloc; 0 runs on analysis_data\n",
    "roi_size = 50       # default number of voxels to define individual roi == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from os.path import join as pjoin, split as psplit\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from statistics import mean\n",
    "import csv\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import warnings as warn\n",
    "import operator\n",
    "import itertools\n",
    "from nipype.interfaces import fsl\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "import nilearn\n",
    "from nilearn.masking import apply_mask\n",
    "from nilearn import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject look up table conversion (IGNORING undescores)\n",
    "def Convert_Subname(Newname):\n",
    "    tmp_root = '/om/user/rezzo/Subject_Conversion_Table.csv'\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \",\"):\n",
    "            if Newname == line[0]:\n",
    "                Oldname = line[1]\n",
    "            else:\n",
    "                continue\n",
    "    return Oldname  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain values and indices of individual rois\n",
    "def nan_largestval(ary, n):\n",
    "    flat = ary.flatten()                      # transform to one array\n",
    "    values = -np.sort(-flat)                  # order values greatest to least, nans at the end\n",
    "    idx = (-flat).argsort()[:n]               # obtain indices of the values in flat array\n",
    "    idx2 = np.unravel_index(idx, ary.shape)   # transform indices to original array\n",
    "    return [values[0:n], idx2]                # return values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain the count for the lateralization calculations\n",
    "def nan_abovethresh(ary, thresh):\n",
    "    flat = ary.flatten()                     # transform to one array\n",
    "    count = np.sum(flat<thresh)              # count voxels greater than thresh\n",
    "    return [count]                           # return count                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to return a list of floats from a text file  \n",
    "def txt2list(file,details):\n",
    "    txtfile = open(file,'r')\n",
    "    details = []\n",
    "    for line in txtfile:\n",
    "        details.extend([float(i) for i in line.rstrip('\\n').split()])\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to transform pearson's R to fisher's Z\n",
    "def pearson2fisher(pearsonR):\n",
    "    fisherZ = 0.5*(np.log(1+pearsonR) - np.log(1- pearsonR)) # np.log is NATURAL LOG!*\n",
    "    return fisherZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to cut a substring out of a larger string\n",
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out first element of design file\n",
    "def Design_file(tmp_root):\n",
    "    array = []\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \"\\t\"):\n",
    "            array.append(line[3])\n",
    "    if array[1] == 'belief':\n",
    "        return \"1\"\n",
    "    elif array[1] == 'photo':\n",
    "        return \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files in directory and obtain a list of the file names\n",
    "def roi_list(directory_name):\n",
    "    full_list = os.listdir(directory_name)\n",
    "    for element in range(0, len(full_list)):\n",
    "        #replace = re.match(\"(.*?)_\",full_list[element]).group()\n",
    "        replace = full_list[element].split('_')[0]\n",
    "        full_list[element] = replace\n",
    "    return full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervals(start_num, N):\n",
    "    init = []\n",
    "    counter = -1\n",
    "    for el in range(0, N):\n",
    "        counter = counter + 1\n",
    "        init.append(start_num + counter)\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create the appropriate list of lateral rois\n",
    "def lat_roi_condense(roi_list):\n",
    "    for elements in range(0, len(roi_list)):\n",
    "        roi_list[elements] = roi_list[elements][1:]\n",
    "    return list(set(roi_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create roi_indexes, and subject indexes\n",
    "\n",
    "if pilot is 1:\n",
    "    main_root = '/om/user/rezzo/OpenAutism/pilot_data/'\n",
    "else:\n",
    "    main_root = '/om/user/rezzo/OpenAutism/analysis_data/'\n",
    "\n",
    "roiroot = main_root+'MASKS/STANDARD_MASKS/'\n",
    "biroiroot = main_root+'MASKS/LATERAL_MASKS/'\n",
    "roi_index = roi_list(roiroot)\n",
    "lat_index = lat_roi_condense(roi_list(biroiroot))\n",
    "#lat_index = lat_roi_condense(lat_index)\n",
    "    \n",
    "all_subjects = glob(main_root+\"/SUBJECTS/*/\")\n",
    "subject_list = []\n",
    "\n",
    "# create list of subject in folder\n",
    "for subs in range(0, len(all_subjects)):\n",
    "    m = re.search('SAX_OA_(.+?)/', all_subjects[subs])\n",
    "    if m:\n",
    "        found = m.group(1)\n",
    "        subject_list.append('SAX_OA_'+found) # subject is the list with all subject names.\n",
    "\n",
    "# Here is the sorted list of OA subject names        \n",
    "subject_list.sort\n",
    "\n",
    "# load list of subjects to include after motion outlier exclusion\n",
    "good_subjects = main_root+'MOTION_INFO/Runs_LenientMotionFiltered.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/neuro/lib/python3.6/site-packages/ipykernel_launcher.py:80: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAX_OA_001_tomloc001\n",
      "SAX_OA_001_tomloc002\n"
     ]
    }
   ],
   "source": [
    "######## MAGNITUDE AND LATERALIZATION ##########\n",
    "# NOTE: this section identifies and excludes motion outliers and performs run by run\n",
    "# this is redundant if these measures have been priorly performed\n",
    "\n",
    "subject_list = ['SAX_OA_001']\n",
    "\n",
    "for subject in subject_list:\n",
    "    warn.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
    "    imgroot = main_root+'SUBJECTS/'+subject+'/derivatives/standard/'\n",
    "    temp_list = os.listdir(imgroot+'first_level_analyses')\n",
    "\n",
    "    task_list = []\n",
    "    \n",
    "    # identify number of tasks for this subject\n",
    "    for items in temp_list:\n",
    "        TL = find_between(items, \"tstat1_\",'_run-',) \n",
    "        if TL is not \"\":\n",
    "            task_list.append(TL)\n",
    "        task_list = list(set(task_list)) #remove repeats\n",
    "    \n",
    "        # identify number of runs for this task\n",
    "        for task in task_list:\n",
    "            runs = []\n",
    "            st = find_between(items, \"tstat1_\"+task+'_run-', '.nii.gz') \n",
    "            if st is not \"\":\n",
    "                runs.append(st)\n",
    "                \n",
    "            # for each run in current task\n",
    "            for run in runs:\n",
    "\n",
    "                # check if it is a motion outlier:\n",
    "                if subject+task+run in open(good_subjects).read():\n",
    "                    T_image = os.path.join(imgroot+'first_level_analyses/'+'tstat1_'+task+'_run-'+run+'.nii.gz')\n",
    "                    CON_image = os.path.join(imgroot+'first_level_analyses/'+'cope1_'+task+'_run-'+run+'.nii.gz')\n",
    "                    Z_image = os.path.join(imgroot+'first_level_analyses/zstat1_'+task+'_run-'+run+'.nii.gz')\n",
    "\n",
    "                    # load t-image, z-image, con-image\n",
    "                    t_image = nib.load(T_image)\n",
    "                    z_image = nib.load(Z_image)\n",
    "                    con_image = nib.load(CON_image)\n",
    "\n",
    "                    # convert images to numpy arrays\n",
    "                    t_data = np.array(t_image.dataobj)\n",
    "                    z_data = np.array(z_image.dataobj)\n",
    "                    CON_data = np.array(con_image.dataobj)\n",
    "\n",
    "                    # initialize lists\n",
    "                    mag_per_roi = [[]] * len(roi_index) # ave magnitude\n",
    "                    pos_per_roi = [[]] * len(roi_index) # ave position\n",
    "                    lat_lenient = [[]] * len(lat_index) # lat for low thresh\n",
    "                    count_lenient = [[]] * len(lat_index) # count of total voxels above low thresh\n",
    "                    lat_strict = [[]] * len(lat_index)  # lat for high thresh\n",
    "                    count_strict = [[]] * len(lat_index) # count of total voxels above high thresh\n",
    "\n",
    "                    count = -1\n",
    "\n",
    "                    for roi in roi_index:\n",
    "                        count = count + 1\n",
    "                        parcel_file = os.path.join(data_path, roiroot + roi+ '_FSL_space.nii.gz')\n",
    "                        PARCEL = nib.load(parcel_file)\n",
    "                        binary_data = np.array(PARCEL.dataobj) #this is the mask\n",
    "\n",
    "                        # make all values in binary mask = 0 to NAN\n",
    "                        binary_data[binary_data == 0] = 'nan'\n",
    "\n",
    "                        # save as a flattened array\n",
    "                        roi_con = CON_data*binary_data #just added this\n",
    "                        MVPA_array = roi_con.flatten()\n",
    "                        np.save(imgroot+'second_level_analyses/multivariate/MVPA_array', MVPA_array)\n",
    "\n",
    "                        # multiply t-image by roi masks;\n",
    "                        roi_hs = t_data*binary_data\n",
    "                        roi_hs = roi_hs.astype('float')\n",
    "\n",
    "                        # find top N t-values within this space to define individual's ROI\n",
    "                        [values, indices] = nan_largestval(roi_hs, roi_size)\n",
    "\n",
    "                        # initialize individual mask space\n",
    "                        roi_mask = np.zeros(shape=roi_hs.shape)\n",
    "                        binary_data[binary_data == 'nan'] = 0   # temporary, for saving mask\n",
    "\n",
    "                        # create subject-specific roi mask (N voxels, e.g. 50)\n",
    "                        for hh in range(roi_size):\n",
    "                            roi_mask[indices[0][hh],indices[1][hh],indices[2][hh]] = 1\n",
    "\n",
    "                        # save it as nifti image\n",
    "                        roi_img = nib.Nifti1Image(roi_mask, PARCEL.affine, PARCEL.header)\n",
    "                        fname = pjoin(imgroot+'second_level_analyses/MISC/individual_roi_masks/'+roi+'_indiv_roi_mask_run'+run)\n",
    "                        nib.save(roi_img, fname)\n",
    "\n",
    "                        # calculate average position for subject's run\n",
    "                        X = int(np.mean(indices[0]))\n",
    "                        Y = int(np.mean(indices[1]))\n",
    "                        Z = int(np.mean(indices[2]))\n",
    "                        location = [X, Y, Z]\n",
    "                        pos_per_roi[count] = location      \n",
    "\n",
    "                        #multiply roi_mask defined by highest t-vaues with con image to obtain magnitude of contrast\n",
    "                        roi_mask[roi_mask == 0] = 'nan'\n",
    "                        roi_contrast = CON_data*roi_mask\n",
    "                        means = np.nanmean(roi_contrast)\n",
    "                        mag_per_roi[count] = means # summary stat\n",
    "\n",
    "                        # Save con values selected by the mask as a flat array\n",
    "                        Topvoxels = roi_contrast.flatten()\n",
    "                        Topvoxels = Topvoxels[~np.isnan(Topvoxels)]\n",
    "                        np.save(imgroot+'second_level_analyses/magnitude/'+subject+task+run+'_Top'+str(roi_size)+'voxels_contrast_'+roi, Topvoxels)\n",
    "\n",
    "                    ############################# LATERALIZATION ###############################\n",
    "\n",
    "                    count = -1\n",
    "\n",
    "                    for roi in lat_index:\n",
    "                        count = count + 1\n",
    "\n",
    "                        combined_left = os.path.join(data_path, biroiroot + 'l'+roi+'_lateral_FSL_space.nii.gz')\n",
    "                        combined_right = os.path.join(data_path, biroiroot + 'r'+roi+'_lateral_FSL_space.nii.gz') \n",
    "\n",
    "                        # load parcels\n",
    "                        LEFT_PARCEL = nib.load(combined_left)\n",
    "                        RIGHT_PARCEL = nib.load(combined_right)\n",
    "\n",
    "                        # make into numpy arrays\n",
    "                        left_binary_data = np.array(LEFT_PARCEL.dataobj)\n",
    "                        right_binary_data = np.array(RIGHT_PARCEL.dataobj)\n",
    "\n",
    "                        # make all values in binary mask = 0 to NAN (esp. needed for python 2)\n",
    "                        left_binary_data[left_binary_data == 0] = 'nan'\n",
    "                        right_binary_data[right_binary_data == 0] = 'nan'\n",
    "\n",
    "                        p_values_1 = stats.norm.sf(abs(z_data))   #one-sided\n",
    "                        p_values_2 = stats.norm.sf(abs(z_data))*2 #twosided (using this one)\n",
    "\n",
    "                        #multiply p-values by roi masks;\n",
    "                        left_roi_hs = (p_values_2)*left_binary_data\n",
    "                        right_roi_hs = (p_values_2)*right_binary_data\n",
    "\n",
    "                        #convert to float\n",
    "                        left_roi_hs = left_roi_hs.astype('float')\n",
    "                        right_roi_hs = right_roi_hs.astype('float')                  \n",
    "\n",
    "                        thresh1 = 0.01\n",
    "                        thresh2 = 0.001\n",
    "\n",
    "                        ## thresh1 calculations\n",
    "                        #count number of voxels on the left that are p < 0.01\n",
    "                        [total_left1] = nan_abovethresh(left_roi_hs, thresh1)\n",
    "\n",
    "                        #count number of voxels on the right that are p < 0.01\n",
    "                        [total_right1] = nan_abovethresh(right_roi_hs, thresh1)\n",
    "\n",
    "                        count_lenient[count] = total_left1 + total_right1\n",
    "\n",
    "                        ## thresh2 calculations\n",
    "                        #count number of voxels on the left that are p < 0.001\n",
    "                        [total_left2] = nan_abovethresh(left_roi_hs, thresh2)\n",
    "\n",
    "                        #count number of voxels on the right that are p < 0.001\n",
    "                        [total_right2] = nan_abovethresh(right_roi_hs, thresh2)\n",
    "\n",
    "                        count_strict[count] = total_left2 + total_right2\n",
    "\n",
    "                        ## final calculations of lateralization\n",
    "                        # -1 means right dominant and 1 means left dominant, 0 means bilaterial\n",
    "\n",
    "                        # for p < 0.01\n",
    "                        if float(total_left1 + total_right1) == 0:\n",
    "                            lat_lenient[count] = 0.0\n",
    "                        else:\n",
    "                            Lat1 = float(total_left1 - total_right1) / float(total_left1 + total_right1)\n",
    "                            lat_lenient[count] = Lat1\n",
    "                            \n",
    "                        # for p < 0.001\n",
    "                        if float(total_left2 + total_right2) == 0:\n",
    "                            lat_strict[count] = 0.0\n",
    "                        else:\n",
    "                            Lat2 = float(total_left2 - total_right2) / float(total_left2 + total_right2)\n",
    "                            lat_strict[count] = Lat2\n",
    "\n",
    "                    ################## Create the dataframes for all metrics ###############              \n",
    "\n",
    "                    pd.set_option('display.max_colwidth', -1)\n",
    "        \n",
    "                    # dataframe for magnitude\n",
    "                    mag_init = mag_per_roi\n",
    "                    mag_init.extend(['Mean contrast magnitude of the top 50 voxels by t-value'])\n",
    "                    Mdataframe_init = ['MAG_' + s for s in roi_index]\n",
    "                    Mdataframe_init.extend(['Description'])\n",
    "                    Msub_measures = pd.DataFrame([mag_init],\n",
    "                                               columns = Mdataframe_init)\n",
    "                    Msub_measures.rename(index={0: 'Magnitude'})\n",
    "                    \n",
    "                    # dataframe for position\n",
    "                    pos_new = pos_per_roi\n",
    "                    pos_new.extend(['XYZ average position of the top voxels of individual roi based on top t-values'])\n",
    "                    POS_measures = pd.DataFrame([pos_new],\n",
    "                                    columns = Mdataframe_init)\n",
    "                    POS_measures.rename(index={0: 'Position'})\n",
    "                    \n",
    "                    # dataframe for both lateralization indices\n",
    "                    lat_init = lat_lenient\n",
    "                    lat_init2 = lat_strict\n",
    "                    lat_init.extend(['Lateralization index based on voxel selection with p < 0.01 (left count - right count / left and right count)'])\n",
    "                    lat_init2.extend(['Lateralization index based on voxel selection with p < 0.001 (left count - right count / left and right count)'])\n",
    "                    Ldataframe_init = ['LAT_' + s for s in lat_index]\n",
    "                    Ldataframe_init.extend(['Description'])\n",
    "                    Lsub_measures = pd.DataFrame([lat_init],\n",
    "                                                   columns = Ldataframe_init)\n",
    "                    Lsub_measures2 = pd.DataFrame([lat_init2],\n",
    "                                                   columns = Ldataframe_init)\n",
    "                    Lsub_measures.rename(index={0: 'Laterality Index (p < 0.01)'})\n",
    "                    Lsub_measures2.rename(index={0: 'Laterality Index (p < 0.001)'})\n",
    "                        \n",
    "                    ## dataframe for both lateralization counts (total voxels above thresh)\n",
    "                    count_init = count_lenient\n",
    "                    count_init.extend(['Total count based on voxel selection with p < 0.01 (left + right)'])\n",
    "                    count_init2 = count_strict\n",
    "                    count_init2.extend(['Total count based on voxel selection with p < 0.001 (left + right)'])\n",
    "                    count_measures1 = pd.DataFrame([count_init],\n",
    "                                                   columns = Ldataframe_init)\n",
    "                    count_measures2 = pd.DataFrame([count_init2],\n",
    "                                                   columns = Ldataframe_init)\n",
    "\n",
    "                    # save (or append) all dataframes to csv file\n",
    "                    if run == '001':\n",
    "                        Msub_measures.to_csv(imgroot+'second_level_analyses/magnitude/'+subject+'_'+task+'_MAG_SUM_STATS.csv', index=False)\n",
    "                        Lsub_measures.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_SUM_STATS.csv', index=False)\n",
    "                        Lsub_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_SUM_STATS.csv', index=False)\n",
    "                        count_measures1.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_COUNT_STATS.csv', index=False)\n",
    "                        count_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_COUNT_STATS.csv', index=False)\n",
    "                        POS_measures.to_csv(imgroot+'second_level_analyses/MISC/'+subject+'_'+task+'_POSITION_ROI_STATS.csv', index=False)\n",
    "                    else:\n",
    "                        Msub_measures.to_csv(imgroot+'second_level_analyses/magnitude/'+subject+'_'+task+'_MAG_SUM_STATS.csv', mode='a', index=False, header=False)\n",
    "                        Lsub_measures.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_SUM_STATS.csv', mode='a', index=False, header=False)\n",
    "                        Lsub_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_SUM_STATS.csv', mode='a', index=False, header=False)\n",
    "                        count_measures1.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_COUNT_STATS.csv', mode='a', index=False, header=False)\n",
    "                        count_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_COUNT_STATS.csv', mode='a', index=False, header=False)\n",
    "                        POS_measures.to_csv(imgroot+'second_level_analyses/MISC/'+subject+'_'+task+'_POSITION_ROI_STATS.csv', mode='a', index=False, header=False)\n",
    "                        \n",
    "                    # log each completed run on console\n",
    "                    print(subject+'_'+task+run)\n",
    "                        \n",
    "            #####################################################################\n",
    "\n",
    "            # make a list of all the files (these files include each run)\n",
    "            file_list = [imgroot+'second_level_analyses/magnitude/'+subject+'_'+task+'_MAG_SUM_STATS.csv', imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_SUM_STATS.csv',\n",
    "                         imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_SUM_STATS.csv', imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_COUNT_STATS.csv',\n",
    "                        imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_COUNT_STATS.csv']\n",
    "\n",
    "            # now average all metrics runs together, print exception for tasks with no valid runs\n",
    "            for file in file_list:\n",
    "                try:\n",
    "                    df = pd.read_csv(file)\n",
    "                    ex = pd.DataFrame(df.mean())\n",
    "                    ex = ex.transpose()\n",
    "                    ex.to_csv(file[:-4]+'_AVE_RUNS.csv', index=False)\n",
    "                except FileNotFoundError:\n",
    "                    print(\"No Valid Runs for \"+subject+task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomloc001\n"
     ]
    }
   ],
   "source": [
    "############# INTERREGIONAL CORRELATION AND TEMPORAL VARIANCE ###############\n",
    "\n",
    "subject_list = ['SAX_OA_001']\n",
    "\n",
    "# define the experimental conditions for all experiments\n",
    "if pilot == 1:\n",
    "    faces = ['belief'] # dummy face for tomloc\n",
    "    TR = 2\n",
    "else:\n",
    "    faces = [] # need a list of all analysis_data face conditions\n",
    "\n",
    "# define N discrete points for temporal variance (same place relative to hrf across trials)\n",
    "DISCRETE_POINTS = 4\n",
    "\n",
    "for subject in subject_list:\n",
    "    warn.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
    "    imgroot = main_root+'SUBJECTS/'+subject+'/derivatives/standard/'\n",
    "    temp_list = os.listdir(imgroot+'first_level_analyses/BOLD_data')\n",
    "    task_list = []\n",
    "    \n",
    "    # figure out how many tasks in subject folder\n",
    "    for items in temp_list:\n",
    "        TL = find_between(items, \"temporaldata_\",'_run-',) \n",
    "        if TL is not \"\":\n",
    "            task_list.append(TL)\n",
    "        task_list = list(set(task_list)) #remove repeats\n",
    "    \n",
    "        # final task list\n",
    "        for task in task_list:\n",
    "            runs = []\n",
    "            st = find_between(items, \"temporaldata_\"+task+'_run-', '.nii.gz') \n",
    "            if st is not \"\":\n",
    "                runs.append(st)\n",
    "            \n",
    "            # need to determine if block or event-related for analysis_data\n",
    "            \n",
    "            # need to figure out TR for analysis_data\n",
    "\n",
    "            # for each run in current task\n",
    "            for run in runs:\n",
    "\n",
    "                Ztrans_values = []\n",
    "                \n",
    "                # check if it is a motion outlier:\n",
    "                if subject+task+run in open(good_subjects).read():\n",
    "                    \n",
    "                    ######################### TEMPORAL VARIANCE ######################\n",
    "                    # read in design file\n",
    "                    fpath = imgroot+'first_level_analyses/BOLD_data/design_'+task+'_run-'+run+'.tsv'\n",
    "                    designfile = pd.read_csv(fpath, sep='\\t')\n",
    "                    \n",
    "                    # these lists will separate each face condition within a run\n",
    "                    onset_list = [[]*len(faces)]\n",
    "                    shifted_onsets = [[]*len(faces)]\n",
    "                    duration_list = [[]*len(faces)]\n",
    "                    count = -1\n",
    "                    \n",
    "                    for element in faces:\n",
    "                        # create list specific to each face condition within a trial\n",
    "                        count = count + 1\n",
    "                        onset_list[count] = designfile.loc[designfile[designfile['trial_type'] == element].index.tolist(),'onset']\n",
    "                        onset_list[count] = [ np.round(x) for x in onset_list[count]] \n",
    "                        duration_list[count] = designfile.loc[designfile[designfile['trial_type'] == element].index.tolist(),'duration']\n",
    "                        # the TR index\n",
    "                        shifted_onsets[count] = (designfile.loc[designfile[designfile['trial_type'] == element].index.tolist(),'onset'] + TR) / TR\n",
    "                        shifted_onsets[count] = [ np.round(x) for x in shifted_onsets[count]] # offset rounded.\n",
    "                        shifted_onsets[count] = [ int(x) for x in shifted_onsets[count]]\n",
    "                        # flatten all lists (treating all face conditions as one condition)\n",
    "                        onset_list = [item for sublist in onset_list for item in sublist]\n",
    "                        duration_list = [item for sublist in duration_list for item in sublist]\n",
    "                        shifted_onsets = [item for sublist in shifted_onsets for item in sublist]\n",
    "                        \n",
    "                        n_items = len(shifted_onsets) # N face conditions in this run\n",
    "                        \n",
    "                    final_exp_points = [[]]*n_items # initialize list for N arrays\n",
    "                      \n",
    "                    # this array will contain subarrays (temporal indices)\n",
    "                    for element in range(0, n_items):\n",
    "                        temp =[]\n",
    "                        temp = intervals(shifted_onsets[element], DISCRETE_POINTS)\n",
    "                        final_exp_points[element]= temp\n",
    "\n",
    "                    # load in 4D functional data of current run\n",
    "                    Time_image = os.path.join(imgroot+'first_level_analyses/BOLD_data/temporaldata_'+task+'_run-'+run+'.nii.gz')\n",
    "                    time_image = nib.load(Time_image)\n",
    "                    time_data = np.array(time_image.dataobj)\n",
    "\n",
    "                    # initialize arrays related to roi\n",
    "                    roicount = -1\n",
    "                    temporal_arrays = [[]] * len(roi_index) # will contain mean temporal array of roi\n",
    "                    TEMPORAL_VAR = [] # variable of interest (final temporal variance for this run)\n",
    "                    \n",
    "                    for roi in roi_index:\n",
    "                        roicount = roicount + 1\n",
    "                        test = []\n",
    "                        # initializing\n",
    "                        final_exp_values = [[]]*len(final_exp_points)\n",
    "                        std_exp_values = [[]]*len(final_exp_points)\n",
    "                        std_mean_values = [[]]*len(final_exp_points)\n",
    "                        \n",
    "                        # load in individual roi mask\n",
    "                        roi_file = os.path.join(data_path, imgroot+'second_level_analyses/MISC/individual_roi_masks/'+roi+'_indiv_roi_mask_run'+run+'.nii')\n",
    "                        ROI = nib.load(roi_file)\n",
    "                        binary_data = np.array(ROI.dataobj)\n",
    "\n",
    "                        # use fsl to extract mean signal in roi: \n",
    "                        meants = fsl.ImageMeants(in_file=imgroot+'first_level_analyses/BOLD_data/temporaldata_'+task+'_run-'+run+'.nii.gz', \n",
    "                                                 mask=imgroot+'second_level_analyses/MISC/individual_roi_masks/'+roi+'_indiv_roi_mask_run'+run+'.nii',\n",
    "                                                 out_file=imgroot+'second_level_analyses/MISC/mean_roi_Temporal_Signal/Mean_temporal_signal_'+roi+'_'+task+'_run-'+run+'.txt')\n",
    "\n",
    "                        meants.cmdline\n",
    "                        meants.run()\n",
    "\n",
    "                        # save the mean signal as a list for each roi\n",
    "                        temporal_arrays[roicount] = txt2list(imgroot+'second_level_analyses/MISC/mean_roi_Temporal_Signal/Mean_temporal_signal_'+roi+'_'+task+'_run-'+run+'.txt', test)\n",
    "\n",
    "                        # identifying values for temporal var computation\n",
    "                        for item in range(0, len(final_exp_points)):\n",
    "                            templist = []\n",
    "                            for points in range (0, DISCRETE_POINTS):\n",
    "                                templist.append(temporal_arrays[roicount][(final_exp_points[item][points])-1]) # accounting for 0 index\n",
    "                            final_exp_values[item] = templist         # BOLD values of the temporal indices\n",
    "                            std_exp_values[item] = np.std(templist)   # std of each trial\n",
    "                            std_mean_values[item] = np.mean(std_exp_values[item]) # mean of all stds\n",
    "                            \n",
    "                        # save variance per roi\n",
    "                        TEMPORAL_VAR.append(std_mean_values)\n",
    "                        \n",
    "                    ########################## INTERREGIONAL CORRELATION #########################################################\n",
    "\n",
    "                    # make a correlation matrix for all rois' mean signal arrays\n",
    "                    COR_MATRIX = np.corrcoef(temporal_arrays)                   \n",
    "\n",
    "                    # save this per subject per run\n",
    "                    np.save(imgroot+'second_level_analyses/interregional_cor/InterregionCor_'+task+'_run-'+run,COR_MATRIX)\n",
    "\n",
    "                    # need to z-transform the R correlations \n",
    "                    ZCorrMatrix = pearson2fisher(COR_MATRIX)\n",
    "                    np.save(imgroot+'second_level_analyses/interregional_cor/InterregionCor_'+task+'_run-'+run,ZCorrMatrix)  \n",
    "\n",
    "                    # use a mask to convert diagonal elements to nans\n",
    "                    mask = np.ones(ZCorrMatrix.shape, dtype=bool)\n",
    "                    np.fill_diagonal(ZCorrMatrix, 'nan')\n",
    "\n",
    "                    # calculate mean ignoring nans\n",
    "                    Ztrans_values.append(ZCorrMatrix[~np.isnan(ZCorrMatrix)].mean())\n",
    "                \n",
    "                # if run is never found in inclusion list, will not run analysis\n",
    "                else:\n",
    "                    print(subject+task+run+' is a motion outlier')\n",
    "                    \n",
    "                pd.set_option('display.max_colwidth', -1)\n",
    "                \n",
    "                ############ CREATING AND SAVING DATAFRAMES #############\n",
    "                \n",
    "                # Create dataframe for temporal variance \n",
    "                dataframe_meas = [Ztrans_values, ['Z-transformed score of the mean pearsons R of correlation matrix relating each ROIs activation (excludes diagonal)']]\n",
    "                dataframe_MEAS = np.concatenate(dataframe_meas).ravel()\n",
    "                dataframe_COL = ['INTERREGION_COR', 'Description']\n",
    "                SUB_measures = pd.DataFrame([dataframe_MEAS],\n",
    "                                         columns=dataframe_COL)\n",
    "                SUB_measures.rename(index={0: 'Correlation_Z_transf'})\n",
    "                \n",
    "                # Create dataframe for temporal variance \n",
    "                tempv_0 = [item[0] for item in TEMPORAL_VAR]\n",
    "                tempv_0.extend(['temporal variance within subject, within roi, for '+faces[0]+' condition'])\n",
    "                dataframe_meas1 = [tempv_0]\n",
    "                dataframe_MEA1 = np.concatenate(dataframe_meas1).ravel()\n",
    "                dataframe_col2 = ['TEMPVAR_FACES_' + s for s in roi_index]\n",
    "                dataframe_col2.extend(['Description'])\n",
    "                sub_measures2 = pd.DataFrame([dataframe_MEA1],\n",
    "                        columns=dataframe_col2)\n",
    "                \n",
    "                # save dataframes as csv files\n",
    "                if run == '001':\n",
    "                    SUB_measures.to_csv(imgroot+'second_level_analyses/interregional_cor/'+subject+'_'+task+'_INTERREG_SUM_STATS.csv', index=False)\n",
    "                    sub_measures2.to_csv(imgroot+'second_level_analyses/temporal_variance/'+subject+'_'+task+'_TEMPNOISE_SUM_STATS.csv', index=False)\n",
    "                else:\n",
    "                    SUB_measures.to_csv(imgroot+'second_level_analyses/interregional_cor/'+subject+'_'+task+'_INTERREG_SUM_STATS.csv', mode= 'a', index=False, header=False)\n",
    "                    sub_measures2.to_csv(imgroot+'second_level_analyses/temporal_variance/'+subject+'_'+task+'_TEMPNOISE_SUM_STATS.csv', mode= 'a', index=False, header=False)\n",
    "                     \n",
    "            file_list = [imgroot+'second_level_analyses/interregional_cor/'+subject+'_'+task+'_INTERREG_SUM_STATS.csv',\n",
    "                    imgroot+'second_level_analyses/temporal_variance/'+subject+'_'+task+'_TEMPNOISE_SUM_STATS.csv'] \n",
    "            \n",
    "            for file in file_list:\n",
    "                # now average all runs together\n",
    "                try:\n",
    "                    df = pd.read_csv(file, header=None)\n",
    "                    ex = pd.DataFrame(df.mean())\n",
    "                    ex = ex.transpose()\n",
    "                    ex.to_csv(file[:-4]+'_final.csv', index=False)\n",
    "                except FileNotFoundError:\n",
    "                    print(\"No Valid Runs for \"+subject+task)\n",
    "\n",
    "    print(task+run)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
