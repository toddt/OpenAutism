{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERAL ROI INFORMATION:\n",
    "# This script uses functional roi niftis (converted from.img files and scaled in FSL to correct\n",
    "# size in each dimension). These parcels are used as binary masks on output from first-level \n",
    "# analysis. \n",
    "# This yields a hypothesis space for each roi (same location for every \n",
    "# subject). These ROI's are face parcels from the Kanwisher website http://web.mit.edu/bcs/nklab/GSS.shtml\n",
    "# with the exception of DMFPC and AMY (taken from saxelab).\n",
    "\n",
    "## MAGNITUDE\n",
    "# The top N (default = 50) voxels are then selected from these hypothesis space based on highest t-statistic\n",
    "# (no threshold, and no constraint on contiguity). This defines the ROIs per subject.\n",
    "# The corresponding con values are averaged as the mean magnitude summary statistic.\n",
    "# Also computes average position\n",
    "\n",
    "## LATERALIZATION\n",
    "# Different MASKS used. Masks created by summing hypothesis space and flipped opposite space, \n",
    "# e.g. rTPJ computed as voxels within rTPJ parcel and flipped lTPJ. See commands:\n",
    "# TO FLIP: fslswapdim data -x y z flip_data\n",
    "# TO ADD: fslmaths mask1 -add mask2 -bin output ... (e.g. mask1 = rTPJ original, mask 2 = lTPJ flipped)\n",
    "# Count NUMBER of voxels above a certain p value (0.01 or 0.001) to do calculations: L - R / L + R\n",
    "\n",
    "## INTERREGIONAL CORRELATIONS\n",
    "# Takes mean temporal signal in each roi and creates a pearson's r correlation matrix. Converts \n",
    "# this to Fisherman's Z, and then averages for a summary statistic. Does this per run, and saves\n",
    "# both run data and averaged run data as separate csv files.\n",
    "\n",
    "## TEMPORAL NOISE\n",
    "# Details in section. Looks at specified number of points relative to hrf, and uses these\n",
    "# indices to extract N BOLD values for each trial; finds std for each trial, then averages \n",
    "# across face conditions. Does this for each run, saves runs separately and the mean summary in csv.\n",
    "\n",
    "## MULTIVARIATE VECTORS\n",
    "# This is computed over entire hypothesis space, as a vector of all contrast values within the parcel.\n",
    "\n",
    "# SAVES: \n",
    "# roi_individual_masks & roi_contrast_values (top N voxels per subject), \n",
    "# and text files per metric computations that can be opened as a dataframe.\n",
    "\n",
    "# MASKS DIRECTORY\n",
    "# /om/group/saxelab/OpenAutism/TIER_OpenAutism/analysis_data/MASKS/STANDARD_MASKS\n",
    "# /om/group/saxelab/OpenAutism/TIER_OpenAutism/analysis_data/MASKS/LATERAL_MASKS\n",
    "\n",
    "# Note : to convert Analyze format to Nifti format:\n",
    "# load in header, image (and .mat) files into the folder\n",
    "# fslchfiletype NIFTI_GZ RTPJ_xyz.nii.gz RTPJ_xyz\n",
    "\n",
    "# SETTINGS\n",
    "pilot = 1           # pilot == 1 runs on tomloc; 0 runs on analysis_data\n",
    "roi_size = 50       # default number of voxels to define individual roi == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from os.path import join as pjoin, split as psplit\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from statistics import mean\n",
    "import csv\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import warnings as warn\n",
    "import operator\n",
    "import itertools\n",
    "from nipype.interfaces import fsl\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "import nilearn\n",
    "from nilearn.masking import apply_mask\n",
    "from nilearn import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject look up table conversion (IGNORING undescores)\n",
    "def Convert_Subname(Newname):\n",
    "    tmp_root = '/om/user/rezzo/Subject_Conversion_Table.csv'\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \",\"):\n",
    "            if Newname == line[0]:\n",
    "                Oldname = line[1]\n",
    "            else:\n",
    "                continue\n",
    "    return Oldname  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain values and indices of individual rois\n",
    "def nan_largestval(ary, n):\n",
    "    flat = ary.flatten()                      # transform to one array\n",
    "    values = -np.sort(-flat)                  # order values greatest to least, nans at the end\n",
    "    idx = (-flat).argsort()[:n]               # obtain indices of the values in flat array\n",
    "    idx2 = np.unravel_index(idx, ary.shape)   # transform indices to original array\n",
    "    return [values[0:n], idx2]                # return values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain the count for the lateralization calculations\n",
    "def nan_abovethresh(ary, thresh):\n",
    "    flat = ary.flatten()                     # transform to one array\n",
    "    count = np.sum(flat<thresh)              # count voxels greater than thresh\n",
    "    return [count]                           # return count                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to return a list of floats from a text file  \n",
    "def txt2list(file,details):\n",
    "    txtfile = open(file,'r')\n",
    "    details = []\n",
    "    for line in txtfile:\n",
    "        details.extend([float(i) for i in line.rstrip('\\n').split()])\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to transform pearson's R to fisher's Z\n",
    "def pearson2fisher(pearsonR):\n",
    "    fisherZ = 0.5*(np.log(1+pearsonR) - np.log(1- pearsonR)) # np.log is NATURAL LOG!*\n",
    "    return fisherZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to cut a substring out of a larger string\n",
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find between but with list input\n",
    "def find_list(lis, str1, str2):\n",
    "    init = []\n",
    "    for el in lis:\n",
    "        init.append(find_between(el, str1,str2))\n",
    "        norepeat = list(set(init))\n",
    "        final = list(filter(None, norepeat)) # fastest\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_tasks(runinfo):\n",
    "    temp = (runinfo['task_name'])\n",
    "    tasks = list(temp)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out first element of design file\n",
    "def Design_file(tmp_root):\n",
    "    array = []\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \"\\t\"):\n",
    "            array.append(line[3])\n",
    "    if array[1] == 'belief':\n",
    "        return \"1\"\n",
    "    elif array[1] == 'photo':\n",
    "        return \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files in directory and obtain a list of the file names\n",
    "def roi_list(directory_name):\n",
    "    full_list = os.listdir(directory_name)\n",
    "    for element in range(0, len(full_list)):\n",
    "        #replace = re.match(\"(.*?)_\",full_list[element]).group()\n",
    "        replace = full_list[element].split('_')[0]\n",
    "        full_list[element] = replace\n",
    "    return full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervals(start_num, N):\n",
    "    init = []\n",
    "    counter = -1\n",
    "    for el in range(0, N):\n",
    "        counter = counter + 1\n",
    "        init.append(start_num + counter)\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_info(subject, task, run, datainfo):\n",
    "    run = int(run[-1])\n",
    "    temp = datainfo.loc[(datainfo[0] == subject) & (datainfo[2] == task) & (datainfo[3] == run)]\n",
    "    #return temp\n",
    "    TR = float(temp[4])\n",
    "    faces = list(temp[5])\n",
    "    blocklist = list(temp[6])\n",
    "    block = blocklist[0]\n",
    "    return TR , faces , block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create the appropriate list of lateral rois\n",
    "def lat_roi_condense(roi_list):\n",
    "    for elements in range(0, len(roi_list)):\n",
    "        roi_list[elements] = roi_list[elements][1:]\n",
    "    return list(set(roi_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create roi_indexes, and subject indexes\n",
    "\n",
    "if pilot is 1:\n",
    "    main_root = '/om/group/saxelab/OpenAutism/TIER_OpenAutism/pilot_data/'\n",
    "else:\n",
    "    main_root = '/om/group/saxelab/OpenAutism/TIER_OpenAutism/analysis_data/'\n",
    "\n",
    "roiroot = main_root+'MASKS/STANDARD_MASKS/'\n",
    "biroiroot = main_root+'MASKS/LATERAL_MASKS/'\n",
    "roi_index = roi_list(roiroot)\n",
    "lat_index = lat_roi_condense(roi_list(biroiroot))\n",
    "#lat_index = lat_roi_condense(lat_index)\n",
    "    \n",
    "all_subjects = glob(main_root+\"/SUBJECTS/*/\")\n",
    "subject_list = []\n",
    "\n",
    "# create list of subject in folder\n",
    "for subs in range(0, len(all_subjects)):\n",
    "    m = re.search('SAX_OA_(.+?)/', all_subjects[subs])\n",
    "    if m:\n",
    "        found = m.group(1)\n",
    "        subject_list.append('SAX_OA_'+found) # subject is the list with all subject names.\n",
    "\n",
    "# Here is the sorted list of OA subject names        \n",
    "subject_list.sort\n",
    "\n",
    "# load list of subjects to include after motion outlier exclusion\n",
    "good_subjects = main_root+'RUN_INFO/Runs_LenientMotionFiltered.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAX_OA_006_tomloc002\n",
      "SAX_OA_006_tomloc001 excluded due to motion\n",
      "SAX_OA_003_tomloc002\n",
      "SAX_OA_003_tomloc001\n",
      "SAX_OA_098_tomloc002\n",
      "SAX_OA_098_tomloc001\n",
      "SAX_OA_009_tomloc002\n",
      "SAX_OA_009_tomloc001\n",
      "SAX_OA_005_tomloc002\n",
      "SAX_OA_005_tomloc001\n",
      "SAX_OA_032_tomloc002\n",
      "SAX_OA_032_tomloc001\n",
      "SAX_OA_012_tomloc002\n",
      "SAX_OA_012_tomloc003\n",
      "SAX_OA_012_tomloc004\n",
      "SAX_OA_012_tomloc001\n",
      "SAX_OA_008_tomloc002\n",
      "SAX_OA_008_tomloc001\n",
      "SAX_OA_004_tomloc002\n",
      "SAX_OA_004_tomloc001\n",
      "SAX_OA_081_tomloc002\n",
      "SAX_OA_081_tomloc001\n",
      "SAX_OA_049_tomloc002\n",
      "SAX_OA_049_tomloc001\n",
      "SAX_OA_016_tomloc002\n",
      "SAX_OA_016_tomloc001\n",
      "SAX_OA_101_tomloc002\n",
      "SAX_OA_101_tomloc001\n",
      "SAX_OA_069_tomloc002\n",
      "SAX_OA_069_tomloc001\n",
      "SAX_OA_096_tomloc002 excluded due to motion\n",
      "SAX_OA_096_tomloc001 excluded due to motion\n",
      "SAX_OA_108_tomloc002\n",
      "SAX_OA_108_tomloc001\n",
      "SAX_OA_083_tomloc002\n",
      "SAX_OA_083_tomloc001\n",
      "SAX_OA_036_tomloc002\n",
      "SAX_OA_036_tomloc001\n",
      "SAX_OA_038_tomloc002\n",
      "SAX_OA_038_tomloc001\n",
      "SAX_OA_109_tomloc002\n",
      "SAX_OA_109_tomloc001\n",
      "SAX_OA_111_tomloc002\n",
      "SAX_OA_111_tomloc001\n",
      "SAX_OA_011_tomloc002\n",
      "SAX_OA_011_tomloc001\n",
      "SAX_OA_041_tomloc002\n",
      "SAX_OA_041_tomloc001\n",
      "SAX_OA_040_tomloc002 excluded due to motion\n",
      "SAX_OA_040_tomloc001\n",
      "SAX_OA_044_tomloc002 excluded due to motion\n",
      "SAX_OA_044_tomloc001 excluded due to motion\n",
      "SAX_OA_099_tomloc002\n",
      "SAX_OA_099_tomloc001\n",
      "SAX_OA_054_tomloc002\n",
      "SAX_OA_054_tomloc001\n",
      "SAX_OA_075_tomloc002\n",
      "SAX_OA_075_tomloc001\n",
      "SAX_OA_082_tomloc002\n",
      "SAX_OA_082_tomloc001\n",
      "SAX_OA_034_tomloc002 excluded due to motion\n",
      "SAX_OA_034_tomloc003 excluded due to motion\n",
      "SAX_OA_034_tomloc004 excluded due to motion\n",
      "SAX_OA_034_tomloc001 excluded due to motion\n",
      "SAX_OA_051_tomloc002\n",
      "SAX_OA_051_tomloc001\n",
      "SAX_OA_028_tomloc002\n",
      "SAX_OA_028_tomloc003\n",
      "SAX_OA_028_tomloc004\n",
      "SAX_OA_028_tomloc001\n",
      "SAX_OA_048_tomloc002\n",
      "SAX_OA_048_tomloc001\n",
      "SAX_OA_027_tomloc002\n",
      "SAX_OA_027_tomloc001\n",
      "SAX_OA_061_tomloc002\n",
      "SAX_OA_061_tomloc001\n",
      "SAX_OA_026_tomloc002\n",
      "SAX_OA_026_tomloc001\n",
      "SAX_OA_015_tomloc002 excluded due to motion\n",
      "SAX_OA_015_tomloc001 excluded due to motion\n",
      "SAX_OA_037_tomloc002\n",
      "SAX_OA_037_tomloc001\n",
      "SAX_OA_078_tomloc002\n",
      "SAX_OA_078_tomloc001\n",
      "SAX_OA_042_tomloc002\n",
      "SAX_OA_042_tomloc001\n",
      "SAX_OA_056_tomloc002\n",
      "SAX_OA_056_tomloc001\n",
      "SAX_OA_022_tomloc002\n",
      "SAX_OA_022_tomloc001\n",
      "SAX_OA_094_tomloc002 excluded due to motion\n",
      "SAX_OA_094_tomloc001\n",
      "SAX_OA_033_tomloc002 excluded due to motion\n",
      "SAX_OA_033_tomloc001 excluded due to motion\n",
      "SAX_OA_091_tomloc002 excluded due to motion\n",
      "SAX_OA_091_tomloc001 excluded due to motion\n",
      "SAX_OA_074_tomloc002\n",
      "SAX_OA_074_tomloc001\n",
      "SAX_OA_085_tomloc002\n",
      "SAX_OA_085_tomloc001\n",
      "SAX_OA_047_tomloc002\n",
      "SAX_OA_047_tomloc001\n",
      "SAX_OA_055_tomloc002 excluded due to motion\n",
      "SAX_OA_055_tomloc003\n",
      "SAX_OA_055_tomloc004\n",
      "SAX_OA_055_tomloc001\n",
      "SAX_OA_002_tomloc001\n",
      "SAX_OA_080_tomloc002 excluded due to motion\n",
      "SAX_OA_080_tomloc001 excluded due to motion\n",
      "SAX_OA_030_tomloc002\n",
      "SAX_OA_030_tomloc001\n",
      "SAX_OA_064_tomloc002 excluded due to motion\n",
      "SAX_OA_064_tomloc003 excluded due to motion\n",
      "SAX_OA_064_tomloc004 excluded due to motion\n",
      "SAX_OA_064_tomloc001 excluded due to motion\n",
      "SAX_OA_029_tomloc002\n",
      "SAX_OA_029_tomloc003\n",
      "SAX_OA_029_tomloc004\n",
      "SAX_OA_029_tomloc001\n",
      "SAX_OA_025_tomloc002\n",
      "SAX_OA_025_tomloc001\n",
      "SAX_OA_097_tomloc002\n",
      "SAX_OA_097_tomloc001\n",
      "SAX_OA_104_tomloc002 excluded due to motion\n",
      "SAX_OA_104_tomloc001 excluded due to motion\n",
      "SAX_OA_018_tomloc002\n",
      "SAX_OA_018_tomloc001\n",
      "SAX_OA_084_tomloc002\n",
      "SAX_OA_084_tomloc001\n",
      "SAX_OA_100_tomloc002 excluded due to motion\n",
      "SAX_OA_100_tomloc001 excluded due to motion\n",
      "SAX_OA_050_tomloc002\n",
      "SAX_OA_050_tomloc001\n",
      "SAX_OA_031_tomloc002\n",
      "SAX_OA_031_tomloc003\n",
      "SAX_OA_031_tomloc004\n",
      "SAX_OA_031_tomloc001\n",
      "SAX_OA_001_tomloc002\n",
      "SAX_OA_001_tomloc001\n",
      "SAX_OA_073_tomloc002\n",
      "SAX_OA_073_tomloc001\n",
      "SAX_OA_057_tomloc002\n",
      "SAX_OA_057_tomloc001\n",
      "SAX_OA_095_tomloc002 excluded due to motion\n",
      "SAX_OA_095_tomloc001\n",
      "SAX_OA_076_tomloc002\n",
      "SAX_OA_076_tomloc001\n",
      "SAX_OA_058_tomloc002\n",
      "SAX_OA_058_tomloc001\n",
      "SAX_OA_010_tomloc002\n",
      "SAX_OA_010_tomloc001\n",
      "SAX_OA_013_tomloc002\n",
      "SAX_OA_013_tomloc001 excluded due to motion\n",
      "SAX_OA_052_tomloc002\n",
      "SAX_OA_052_tomloc001\n",
      "SAX_OA_106_tomloc002 excluded due to motion\n",
      "SAX_OA_106_tomloc001 excluded due to motion\n",
      "SAX_OA_068_tomloc002\n",
      "SAX_OA_068_tomloc001\n",
      "SAX_OA_066_tomloc002\n",
      "SAX_OA_066_tomloc001\n",
      "SAX_OA_020_tomloc002\n",
      "SAX_OA_020_tomloc001\n",
      "SAX_OA_017_tomloc002\n",
      "SAX_OA_017_tomloc001\n",
      "SAX_OA_039_tomloc002\n",
      "SAX_OA_039_tomloc001\n",
      "SAX_OA_105_tomloc002\n",
      "SAX_OA_105_tomloc001\n",
      "SAX_OA_070_tomloc002\n",
      "SAX_OA_070_tomloc001\n",
      "SAX_OA_063_tomloc002\n",
      "SAX_OA_063_tomloc003\n",
      "SAX_OA_063_tomloc004\n",
      "SAX_OA_063_tomloc001\n",
      "SAX_OA_053_tomloc002\n",
      "SAX_OA_053_tomloc001\n",
      "SAX_OA_102_tomloc002\n",
      "SAX_OA_102_tomloc001\n",
      "SAX_OA_046_tomloc002\n",
      "SAX_OA_046_tomloc001\n",
      "SAX_OA_103_tomloc002\n",
      "SAX_OA_103_tomloc001\n",
      "SAX_OA_086_tomloc002\n",
      "SAX_OA_086_tomloc001\n",
      "SAX_OA_035_tomloc002\n",
      "SAX_OA_035_tomloc003\n",
      "SAX_OA_035_tomloc004\n",
      "SAX_OA_035_tomloc001\n",
      "SAX_OA_077_tomloc002\n",
      "SAX_OA_077_tomloc001\n",
      "SAX_OA_093_tomloc002\n",
      "SAX_OA_093_tomloc001\n",
      "SAX_OA_065_tomloc002\n",
      "SAX_OA_065_tomloc003\n",
      "SAX_OA_065_tomloc004\n",
      "SAX_OA_065_tomloc001\n",
      "SAX_OA_024_tomloc002\n",
      "SAX_OA_024_tomloc001\n",
      "SAX_OA_023_tomloc002\n",
      "SAX_OA_023_tomloc001\n",
      "SAX_OA_043_tomloc002\n",
      "SAX_OA_043_tomloc001\n",
      "SAX_OA_072_tomloc002\n",
      "SAX_OA_072_tomloc001\n",
      "SAX_OA_079_tomloc002 excluded due to motion\n",
      "SAX_OA_079_tomloc001 excluded due to motion\n",
      "SAX_OA_110_tomloc002\n",
      "SAX_OA_110_tomloc001\n",
      "SAX_OA_062_tomloc002 excluded due to motion\n",
      "SAX_OA_062_tomloc003 excluded due to motion\n",
      "SAX_OA_062_tomloc004 excluded due to motion\n",
      "SAX_OA_062_tomloc001 excluded due to motion\n",
      "SAX_OA_087_tomloc002 excluded due to motion\n",
      "SAX_OA_087_tomloc001 excluded due to motion\n",
      "SAX_OA_060_tomloc002 excluded due to motion\n",
      "SAX_OA_060_tomloc003\n",
      "SAX_OA_060_tomloc004 excluded due to motion\n",
      "SAX_OA_060_tomloc001\n",
      "SAX_OA_021_tomloc002\n",
      "SAX_OA_021_tomloc001\n",
      "SAX_OA_088_tomloc002\n",
      "SAX_OA_088_tomloc001\n",
      "SAX_OA_059_tomloc002\n",
      "SAX_OA_059_tomloc001\n",
      "SAX_OA_007_tomloc002\n",
      "SAX_OA_007_tomloc001\n",
      "SAX_OA_089_tomloc002 excluded due to motion\n",
      "SAX_OA_089_tomloc001 excluded due to motion\n",
      "SAX_OA_019_tomloc001\n",
      "SAX_OA_107_tomloc002\n",
      "SAX_OA_107_tomloc001\n",
      "SAX_OA_092_tomloc002\n",
      "SAX_OA_092_tomloc001\n",
      "SAX_OA_071_tomloc002\n",
      "SAX_OA_071_tomloc001\n"
     ]
    }
   ],
   "source": [
    "######## MAGNITUDE AND LATERALIZATION ##########\n",
    "# NOTE: this section identifies and excludes motion outliers and performs run by run\n",
    "# this is redundant if these measures have been priorly performed but will not affect results\n",
    "\n",
    "#subject_list = ['SAX_OA_006'] \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "for subject in subject_list:\n",
    "    warn.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
    "    imgroot = main_root+'SUBJECTS/'+subject+'/standard/'\n",
    "    temp_list = os.listdir(imgroot+'first_level_analyses')\n",
    "    \n",
    "    # identify number of tasks for this subject\n",
    "    task_list = find_list(temp_list, \"tstat1_\", '_run-')  \n",
    "    \n",
    "    #identify number of runs for this task\n",
    "    for task in task_list:\n",
    "        runs = find_list(temp_list, \"tstat1_\"+task+'_run-', '.nii.gz') \n",
    "        iteration = 0\n",
    "        \n",
    "        # for each run in current task\n",
    "        for run in runs:\n",
    "            # check if it is a motion outlier:\n",
    "            if subject+task+run in open(good_subjects).read():\n",
    "                iteration = iteration + 1\n",
    "                T_image = os.path.join(imgroot+'first_level_analyses/'+'tstat1_'+task+'_run-'+run+'.nii.gz')\n",
    "                CON_image = os.path.join(imgroot+'first_level_analyses/'+'cope1_'+task+'_run-'+run+'.nii.gz')\n",
    "                Z_image = os.path.join(imgroot+'first_level_analyses/zstat1_'+task+'_run-'+run+'.nii.gz')\n",
    "\n",
    "                # load t-image, z-image, con-image\n",
    "                t_image = nib.load(T_image)\n",
    "                z_image = nib.load(Z_image)\n",
    "                con_image = nib.load(CON_image)\n",
    "\n",
    "                # convert images to numpy arrays\n",
    "                t_data = np.array(t_image.dataobj)\n",
    "                z_data = np.array(z_image.dataobj)\n",
    "                CON_data = np.array(con_image.dataobj)\n",
    "                \n",
    "                # initialize lists\n",
    "                mag_per_roi = [[]] * len(roi_index) # ave magnitude\n",
    "                pos_per_roi = [[]] * len(roi_index) # ave position\n",
    "                lat_lenient = [[]] * len(lat_index) # lat for low thresh\n",
    "                count_lenient = [[]] * len(lat_index) # count of total voxels above low thresh\n",
    "                lat_strict = [[]] * len(lat_index)  # lat for high thresh\n",
    "                count_strict = [[]] * len(lat_index) # count of total voxels above high thresh\n",
    "\n",
    "                count = -1\n",
    "                for roi in roi_index:\n",
    "                    count = count + 1\n",
    "                    parcel_file = os.path.join(data_path, roiroot + roi+ '_FSL_space.nii.gz')\n",
    "                    PARCEL = nib.load(parcel_file)\n",
    "                    binary_data = np.array(PARCEL.dataobj) #this is the mask\n",
    "\n",
    "                    # make all values in binary mask = 0 to NAN\n",
    "                    binary_data[binary_data == 0] = 'nan'\n",
    "\n",
    "                    # save as a flattened array\n",
    "                    roi_con = CON_data*binary_data #just added this\n",
    "                    MVPA_array = roi_con.flatten()\n",
    "                    np.save(imgroot+'second_level_analyses/multivariate/MVPA_array', MVPA_array)\n",
    "                    # multiply t-image by roi masks;\n",
    "                    roi_hs = t_data*binary_data\n",
    "                    roi_hs = roi_hs.astype('float')\n",
    "                    # find top N t-values within this space to define individual's ROI\n",
    "                    [values, indices] = nan_largestval(roi_hs, roi_size)\n",
    "                    # initialize individual mask space\n",
    "                    roi_mask = np.zeros(shape=roi_hs.shape)\n",
    "                    binary_data[binary_data == 'nan'] = 0   # temporary, for saving mask\n",
    "\n",
    "                    # create subject-specific roi mask (N voxels, e.g. 50)\n",
    "                    for hh in range(roi_size):\n",
    "                        roi_mask[indices[0][hh],indices[1][hh],indices[2][hh]] = 1\n",
    "                    # save it as nifti image\n",
    "                    roi_img = nib.Nifti1Image(roi_mask, PARCEL.affine, PARCEL.header)\n",
    "                    fname = pjoin(imgroot+'second_level_analyses/MISC/individual_roi_masks/'+roi+'_indiv_roi_mask_run'+run)\n",
    "                    nib.save(roi_img, fname)\n",
    "                    # calculate average position for subject's run\n",
    "                    X = int(np.mean(indices[0]))\n",
    "                    Y = int(np.mean(indices[1]))\n",
    "                    Z = int(np.mean(indices[2]))\n",
    "                    location = [X, Y, Z]\n",
    "                    pos_per_roi[count] = location  \n",
    "                    #multiply roi_mask defined by highest t-vaues with con image to obtain magnitude of contrast\n",
    "                    roi_mask[roi_mask == 0] = 'nan'\n",
    "                    roi_contrast = CON_data*roi_mask\n",
    "                    means = np.nanmean(roi_contrast)\n",
    "                    mag_per_roi[count] = means # summary stat\n",
    "                    # Save con values selected by the mask as a flat array\n",
    "                    Topvoxels = roi_contrast.flatten()\n",
    "                    Topvoxels = Topvoxels[~np.isnan(Topvoxels)]\n",
    "                    np.save(imgroot+'second_level_analyses/magnitude/'+subject+task+run+'_Top'+str(roi_size)+'voxels_contrast_'+roi, Topvoxels)\n",
    "                ############################# LATERALIZATION ###############################\n",
    "                count = -1\n",
    "\n",
    "                for roi in lat_index:\n",
    "                    count = count + 1\n",
    "\n",
    "                    combined_left = os.path.join(data_path, biroiroot + 'l'+roi+'_lateral_FSL_space.nii.gz')\n",
    "                    combined_right = os.path.join(data_path, biroiroot + 'r'+roi+'_lateral_FSL_space.nii.gz') \n",
    "\n",
    "                    # load parcels\n",
    "                    LEFT_PARCEL = nib.load(combined_left)\n",
    "                    RIGHT_PARCEL = nib.load(combined_right)\n",
    "\n",
    "                    # make into numpy arrays\n",
    "                    left_binary_data = np.array(LEFT_PARCEL.dataobj)\n",
    "                    right_binary_data = np.array(RIGHT_PARCEL.dataobj)\n",
    "\n",
    "                    # make all values in binary mask = 0 to NAN (esp. needed for python 2)\n",
    "                    left_binary_data[left_binary_data == 0] = 'nan'\n",
    "                    right_binary_data[right_binary_data == 0] = 'nan'\n",
    "\n",
    "                    p_values_1 = stats.norm.sf(abs(z_data))   #one-sided\n",
    "                    p_values_2 = stats.norm.sf(abs(z_data))*2 #twosided (using this one)\n",
    "\n",
    "                    #multiply p-values by roi masks;\n",
    "                    left_roi_hs = (p_values_2)*left_binary_data\n",
    "                    right_roi_hs = (p_values_2)*right_binary_data\n",
    "\n",
    "                    #convert to float\n",
    "                    left_roi_hs = left_roi_hs.astype('float')\n",
    "                    right_roi_hs = right_roi_hs.astype('float')                  \n",
    "\n",
    "                    thresh1 = 0.01\n",
    "                    thresh2 = 0.001\n",
    "\n",
    "                    ## thresh1 calculations\n",
    "                    #count number of voxels on the left that are p < 0.01\n",
    "                    [total_left1] = nan_abovethresh(left_roi_hs, thresh1)\n",
    "\n",
    "                    #count number of voxels on the right that are p < 0.01\n",
    "                    [total_right1] = nan_abovethresh(right_roi_hs, thresh1)\n",
    "\n",
    "                    count_lenient[count] = total_left1 + total_right1\n",
    "\n",
    "                    ## thresh2 calculations\n",
    "                    #count number of voxels on the left that are p < 0.001\n",
    "                    [total_left2] = nan_abovethresh(left_roi_hs, thresh2)\n",
    "\n",
    "                    #count number of voxels on the right that are p < 0.001\n",
    "                    [total_right2] = nan_abovethresh(right_roi_hs, thresh2)\n",
    "\n",
    "                    count_strict[count] = total_left2 + total_right2\n",
    "\n",
    "                    ## final calculations of lateralization\n",
    "                    # -1 means right dominant and 1 means left dominant, 0 means bilaterial\n",
    "\n",
    "                    # for p < 0.01\n",
    "                    if float(total_left1 + total_right1) == 0:\n",
    "                        lat_lenient[count] = 0.0\n",
    "                    else:\n",
    "                        Lat1 = float(total_left1 - total_right1) / float(total_left1 + total_right1)\n",
    "                        lat_lenient[count] = Lat1\n",
    "\n",
    "                    # for p < 0.001\n",
    "                    if float(total_left2 + total_right2) == 0:\n",
    "                        lat_strict[count] = 0.0\n",
    "                    else:\n",
    "                        Lat2 = float(total_left2 - total_right2) / float(total_left2 + total_right2)\n",
    "                        lat_strict[count] = Lat2\n",
    "\n",
    "                ################## Create the dataframes for all metrics ###############              \n",
    "                pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "                # dataframe for magnitude\n",
    "                mag_init = mag_per_roi\n",
    "                mag_init.extend(['Mean contrast magnitude of the top 50 voxels by t-value'])\n",
    "                Mdataframe_init = ['MAG_' + s for s in roi_index]\n",
    "                Mdataframe_init.extend(['Description'])\n",
    "                Msub_measures = pd.DataFrame([mag_init],\n",
    "                                           columns = Mdataframe_init)\n",
    "                Msub_measures.rename(index={0: 'Magnitude'})\n",
    "\n",
    "                # dataframe for position\n",
    "                pos_new = pos_per_roi\n",
    "                pos_new.extend(['XYZ average position of the top voxels of individual roi based on top t-values'])\n",
    "                POS_measures = pd.DataFrame([pos_new],\n",
    "                                columns = Mdataframe_init)\n",
    "                POS_measures.rename(index={0: 'Position'})\n",
    "\n",
    "                # dataframe for both lateralization indices\n",
    "                lat_init = lat_lenient\n",
    "                lat_init2 = lat_strict\n",
    "                lat_init.extend(['Lateralization index based on voxel selection with p < 0.01 (left count - right count / left and right count)'])\n",
    "                lat_init2.extend(['Lateralization index based on voxel selection with p < 0.001 (left count - right count / left and right count)'])\n",
    "                Ldataframe_init = ['LAT_' + s for s in lat_index]\n",
    "                Ldataframe_init.extend(['Description'])\n",
    "                Lsub_measures = pd.DataFrame([lat_init],\n",
    "                                               columns = Ldataframe_init)\n",
    "                Lsub_measures2 = pd.DataFrame([lat_init2],\n",
    "                                               columns = Ldataframe_init)\n",
    "                Lsub_measures.rename(index={0: 'Laterality Index (p < 0.01)'})\n",
    "                Lsub_measures2.rename(index={0: 'Laterality Index (p < 0.001)'})\n",
    "\n",
    "                ## dataframe for both lateralization counts (total voxels above thresh)\n",
    "                count_init = count_lenient\n",
    "                count_init.extend(['Total count based on voxel selection with p < 0.01 (left + right)'])\n",
    "                count_init2 = count_strict\n",
    "                count_init2.extend(['Total count based on voxel selection with p < 0.001 (left + right)'])\n",
    "                count_measures1 = pd.DataFrame([count_init],\n",
    "                                               columns = Ldataframe_init)\n",
    "                count_measures2 = pd.DataFrame([count_init2],\n",
    "                                               columns = Ldataframe_init)\n",
    "\n",
    "                # save (or append) all dataframes to csv file\n",
    "                if iteration == 1:\n",
    "                    Msub_measures.to_csv(imgroot+'second_level_analyses/magnitude/'+subject+'_'+task+'_MAG_SUM_STATS.csv', mode='w', index=False)\n",
    "                    Lsub_measures.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_SUM_STATS.csv', mode='w', index=False)\n",
    "                    Lsub_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_SUM_STATS.csv', mode='w', index=False)\n",
    "                    count_measures1.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_COUNT_STATS.csv', mode='w', index=False)\n",
    "                    count_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_COUNT_STATS.csv', mode='w', index=False)\n",
    "                    POS_measures.to_csv(imgroot+'second_level_analyses/MISC/'+subject+'_'+task+'_POSITION_ROI_STATS.csv', mode = 'w', index=False)\n",
    "                else:\n",
    "                    Msub_measures.to_csv(imgroot+'second_level_analyses/magnitude/'+subject+'_'+task+'_MAG_SUM_STATS.csv', mode='a', index=False, header=False)\n",
    "                    Lsub_measures.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_SUM_STATS.csv', mode='a', index=False, header=False)\n",
    "                    Lsub_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_SUM_STATS.csv', mode='a', index=False, header=False)\n",
    "                    count_measures1.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_COUNT_STATS.csv', mode='a', index=False, header=False)\n",
    "                    count_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_COUNT_STATS.csv', mode='a', index=False, header=False)\n",
    "                    POS_measures.to_csv(imgroot+'second_level_analyses/MISC/'+subject+'_'+task+'_POSITION_ROI_STATS.csv', mode='a', index=False, header=False)\n",
    "\n",
    "                # log each completed run on console\n",
    "                print(subject+'_'+task+run)\n",
    "                \n",
    "            else:\n",
    "                print(subject+'_'+task+run+' excluded due to motion')\n",
    "        #####################################################################\n",
    "\n",
    "        # make a list of all the files (these files include each run)\n",
    "        file_list = [imgroot+'second_level_analyses/magnitude/'+subject+'_'+task+'_MAG_SUM_STATS.csv', imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_SUM_STATS.csv',\n",
    "                     imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_SUM_STATS.csv', imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_COUNT_STATS.csv',\n",
    "                    imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_COUNT_STATS.csv']\n",
    "\n",
    "        # now average all metrics runs together, print exception for tasks with no valid runs\n",
    "        for file in file_list:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                ex = pd.DataFrame(df.mean())\n",
    "                ex = ex.transpose()\n",
    "                ex.to_csv(file[:-4]+'_AVE_RUNS.csv', mode='w', index=False)\n",
    "            except FileNotFoundError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAX_OA_006tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_096tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_096tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_040tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_044tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_044tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_034tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_034tomloc003 excluding b/c motion outlier\n",
      "SAX_OA_034tomloc004 excluding b/c motion outlier\n",
      "SAX_OA_034tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_015tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_015tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_094tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_033tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_033tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_091tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_091tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_055tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_080tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_080tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_064tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_064tomloc003 excluding b/c motion outlier\n",
      "SAX_OA_064tomloc004 excluding b/c motion outlier\n",
      "SAX_OA_064tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_104tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_104tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_100tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_100tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_095tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_013tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_106tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_106tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_079tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_079tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_062tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_062tomloc003 excluding b/c motion outlier\n",
      "SAX_OA_062tomloc004 excluding b/c motion outlier\n",
      "SAX_OA_062tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_087tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_087tomloc001 excluding b/c motion outlier\n",
      "SAX_OA_060tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_060tomloc004 excluding b/c motion outlier\n",
      "SAX_OA_089tomloc002 excluding b/c motion outlier\n",
      "SAX_OA_089tomloc001 excluding b/c motion outlier\n"
     ]
    }
   ],
   "source": [
    "############# INTERREGIONAL CORRELATION AND TEMPORAL VARIANCE ###############\n",
    "# takes approximately 2 hours to run\n",
    "\n",
    "#subject_list = ['SAX_OA_001']\n",
    "\n",
    "# define the experimental conditions for all experiments\n",
    "if pilot == 1:\n",
    "    runinfo = pd.read_csv('/om/group/saxelab/OpenAutism/TIER_OpenAutism/pilot_data/RUN_INFO/RUN_INFO.txt', sep='\\t', header=None)\n",
    "else:\n",
    "    runinfo = pd.read_csv('/om/group/saxelab/OpenAutism/TIER_OpenAutism/analysis_data/RUN_INFO/RUN_INFO.txt', sep='\\t', header=None)\n",
    "\n",
    "# define N discrete points for temporal variance (same place relative to hrf across trials)\n",
    "DISCRETE_POINTS = 5\n",
    "\n",
    "for subject in subject_list:\n",
    "    warn.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
    "    imgroot = main_root+'SUBJECTS/'+subject+'/standard/'\n",
    "    temp_list = os.listdir(imgroot+'first_level_analyses/BOLD_data')\n",
    "    \n",
    "    # identify number of tasks for this subject\n",
    "    task_list = find_list(temp_list, \"temporaldata_\", '_run-')  \n",
    "    \n",
    "    #identify number of runs for this task\n",
    "    for task in task_list:\n",
    "        runs = find_list(temp_list, \"temporaldata_\"+task+'_run-', '.nii.gz') \n",
    "        iteration = 0\n",
    "\n",
    "        for run in runs:\n",
    "            \n",
    "            # need to determine if block, TR, and conditions\n",
    "            [TR , faces , block] = Run_info(subject, task, run, runinfo)\n",
    "\n",
    "            if task == 'tomloc':\n",
    "                faces = ['belief']\n",
    "\n",
    "            Ztrans_values = []\n",
    "\n",
    "            # check if it is a motion outlier:\n",
    "            if subject+task+run in open(good_subjects).read():          \n",
    "                iteration = iteration + 1\n",
    "                \n",
    "                ######################### TEMPORAL VARIANCE ######################\n",
    "                # read in design file\n",
    "                fpath = imgroot+'first_level_analyses/BOLD_data/design_'+task+'_run-'+run+'.tsv'\n",
    "                designfile = pd.read_csv(fpath, sep='\\t')\n",
    "\n",
    "                # these lists will separate each face condition within a run\n",
    "                onset_list = [[]*len(faces)]\n",
    "                shifted_onsets = [[]*len(faces)]\n",
    "                duration_list = [[]*len(faces)]\n",
    "                count = -1\n",
    "\n",
    "                for element in faces:\n",
    "                    # create list specific to each face condition within a trial\n",
    "                    count = count + 1\n",
    "                    onset_list[count] = designfile.loc[designfile[designfile['trial_type'] == element].index.tolist(),'onset']\n",
    "                    onset_list[count] = [ np.round(x) for x in onset_list[count]] \n",
    "                    duration_list[count] = designfile.loc[designfile[designfile['trial_type'] == element].index.tolist(),'duration']\n",
    "                    # the TR index\n",
    "                    shifted_onsets[count] = (designfile.loc[designfile[designfile['trial_type'] == element].index.tolist(),'onset'] + TR) / TR\n",
    "                    shifted_onsets[count] = [ np.round(x) for x in shifted_onsets[count]] # offset rounded.\n",
    "                    shifted_onsets[count] = [ int(x) for x in shifted_onsets[count]]\n",
    "                    # flatten all lists (treating all face conditions as one condition)\n",
    "                    onset_list = [item for sublist in onset_list for item in sublist]\n",
    "                    duration_list = [item for sublist in duration_list for item in sublist]\n",
    "                    shifted_onsets = [item for sublist in shifted_onsets for item in sublist]\n",
    "\n",
    "                    n_items = len(shifted_onsets) # N face conditions in this run\n",
    "\n",
    "                final_exp_points = [[]]*n_items # initialize list for N arrays\n",
    "\n",
    "                # this array will contain subarrays (temporal indices)\n",
    "                for element in range(0, n_items):\n",
    "                    temp =[]\n",
    "                    temp = intervals(shifted_onsets[element], DISCRETE_POINTS)\n",
    "                    final_exp_points[element]= temp\n",
    "\n",
    "                # load in 4D functional data of current run\n",
    "                Time_image = os.path.join(imgroot+'first_level_analyses/BOLD_data/temporaldata_'+task+'_run-'+run+'.nii.gz')\n",
    "                time_image = nib.load(Time_image)\n",
    "                time_data = np.array(time_image.dataobj)\n",
    "\n",
    "                # initialize arrays related to roi\n",
    "                roicount = -1\n",
    "                temporal_arrays = [[]] * len(roi_index) # will contain mean temporal array of roi\n",
    "                TEMPORAL_VAR = [] # variable of interest (final temporal variance for this run)\n",
    "\n",
    "                for roi in roi_index:\n",
    "                    roicount = roicount + 1\n",
    "                    test = []\n",
    "                    # initializing\n",
    "                    final_exp_values = [[]]*len(final_exp_points)\n",
    "                    std_exp_values = [[]]*len(final_exp_points)\n",
    "                    std_mean_values = [[]]*len(final_exp_points)\n",
    "\n",
    "                    # load in individual roi mask\n",
    "                    roi_file = os.path.join(data_path, imgroot+'second_level_analyses/MISC/individual_roi_masks/'+roi+'_indiv_roi_mask_run'+run+'.nii')\n",
    "                    ROI = nib.load(roi_file)\n",
    "                    binary_data = np.array(ROI.dataobj)\n",
    "\n",
    "                    # use fsl to extract mean signal in roi: \n",
    "                    meants = fsl.ImageMeants(in_file=imgroot+'first_level_analyses/BOLD_data/temporaldata_'+task+'_run-'+run+'.nii.gz', \n",
    "                                             mask=imgroot+'second_level_analyses/MISC/individual_roi_masks/'+roi+'_indiv_roi_mask_run'+run+'.nii',\n",
    "                                             out_file=imgroot+'second_level_analyses/MISC/mean_roi_Temporal_Signal/Mean_temporal_signal_'+roi+'_'+task+'_run-'+run+'.txt')\n",
    "\n",
    "                    meants.cmdline\n",
    "                    meants.run()\n",
    "\n",
    "                    # save the mean signal as a list for each roi\n",
    "                    temporal_arrays[roicount] = txt2list(imgroot+'second_level_analyses/MISC/mean_roi_Temporal_Signal/Mean_temporal_signal_'+roi+'_'+task+'_run-'+run+'.txt', test)\n",
    "\n",
    "                    if block == 'block':\n",
    "                        # identifying values for temporal var computation\n",
    "                        for item in range(0, len(final_exp_points)):\n",
    "                            templist = []\n",
    "                            for points in range (0, DISCRETE_POINTS):\n",
    "                                templist.append(temporal_arrays[roicount][(final_exp_points[item][points])-1]) # accounting for 0 index\n",
    "                            final_exp_values[item] = templist         # BOLD values of the temporal indices\n",
    "                            std_exp_values[item] = np.std(templist)   # std of each trial\n",
    "                        std_mean_values = np.mean(std_exp_values) # mean of all stds\n",
    "                        \n",
    "                        # save variance per roi\n",
    "                        TEMPORAL_VAR.append(std_mean_values)\n",
    "                        \n",
    "                    else:\n",
    "                        TEMPORAL_VAR.append(float('NaN'))\n",
    "\n",
    "                ########################## INTERREGIONAL CORRELATION #########################################################\n",
    "                # make a correlation matrix for all rois' mean signal arrays\n",
    "                COR_MATRIX = np.corrcoef(temporal_arrays)                   \n",
    "\n",
    "                # save this per subject per run\n",
    "                np.save(imgroot+'second_level_analyses/interregional_cor/InterregionCor_R_'+task+'_run-'+run,COR_MATRIX)\n",
    "\n",
    "                # need to z-transform the R correlations \n",
    "                ZCorrMatrix = pearson2fisher(COR_MATRIX)\n",
    "                np.save(imgroot+'second_level_analyses/interregional_cor/InterregionCor_Z_'+task+'_run-'+run,ZCorrMatrix)  \n",
    "\n",
    "                # use a mask to convert diagonal elements to nans\n",
    "                mask = np.ones(ZCorrMatrix.shape, dtype=bool)\n",
    "                np.fill_diagonal(ZCorrMatrix, 'nan')\n",
    "\n",
    "                # calculate mean ignoring nans\n",
    "                Ztrans_values.append(ZCorrMatrix[~np.isnan(ZCorrMatrix)].mean())\n",
    "\n",
    "                pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "                ############ CREATING AND SAVING DATAFRAMES #############\n",
    "\n",
    "                # Create dataframe for interregion correlation\n",
    "                dataframe_meas = [Ztrans_values, ['Z-transformed score of the mean pearsons R of correlation matrix relating each ROIs activation (excludes diagonal)']]\n",
    "                dataframe_MEAS = np.concatenate(dataframe_meas).ravel()\n",
    "                dataframe_COL = ['INTERREGION_COR', 'Description']\n",
    "                SUB_measures = pd.DataFrame([dataframe_MEAS],\n",
    "                                         columns=dataframe_COL)\n",
    "                SUB_measures.rename(index={0: 'Correlation_Z_transf'})\n",
    "\n",
    "                # Create dataframe for temporal variance \n",
    "                tempv_0 = [item for item in TEMPORAL_VAR]\n",
    "                tempv_0.extend(['temporal variance within subject, within roi, for '+faces[0]+' condition'])\n",
    "                dataframe_meas1 = [tempv_0]\n",
    "                dataframe_MEA1 = np.concatenate(dataframe_meas1).ravel()\n",
    "                dataframe_col2 = ['TEMPVAR_FACES_' + s for s in roi_index]\n",
    "                dataframe_col2.extend(['Description'])\n",
    "                sub_measures2 = pd.DataFrame([dataframe_MEA1],\n",
    "                        columns=dataframe_col2)\n",
    "\n",
    "                # save dataframes as csv files\n",
    "                if iteration == 1:\n",
    "                    SUB_measures.to_csv(imgroot+'second_level_analyses/interregional_cor/'+subject+'_'+task+'_INTERREG_SUM_STATS.csv', mode= 'w', index=False)\n",
    "                    sub_measures2.to_csv(imgroot+'second_level_analyses/temporal_variance/'+subject+'_'+task+'_TEMPNOISE_SUM_STATS.csv', mode= 'w', index=False)\n",
    "                else:\n",
    "                    SUB_measures.to_csv(imgroot+'second_level_analyses/interregional_cor/'+subject+'_'+task+'_INTERREG_SUM_STATS.csv', mode= 'a', index=False, header=False)\n",
    "                    sub_measures2.to_csv(imgroot+'second_level_analyses/temporal_variance/'+subject+'_'+task+'_TEMPNOISE_SUM_STATS.csv', mode= 'a', index=False, header=False)\n",
    "                \n",
    "            # if run is never found in inclusion list, will not run analysis\n",
    "            else:\n",
    "                print(subject+task+run+' excluding b/c motion outlier')\n",
    "\n",
    "        file_list = [imgroot+'second_level_analyses/interregional_cor/'+subject+'_'+task+'_INTERREG_SUM_STATS.csv',\n",
    "                imgroot+'second_level_analyses/temporal_variance/'+subject+'_'+task+'_TEMPNOISE_SUM_STATS.csv'] \n",
    "\n",
    "        for file in file_list:\n",
    "            # now average all runs together\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                ex = pd.DataFrame(df.mean())\n",
    "                ex = ex.transpose()\n",
    "                ex.to_csv(file[:-4]+'_AVE_RUNS.csv', mode='w', index=False)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Valid Runs for SAX_OA_096_tomloc\n",
      "No Valid Runs for SAX_OA_096_tomloc\n",
      "No Valid Runs for SAX_OA_096_tomloc\n",
      "No Valid Runs for SAX_OA_096_tomloc\n",
      "No Valid Runs for SAX_OA_044_tomloc\n",
      "No Valid Runs for SAX_OA_044_tomloc\n",
      "No Valid Runs for SAX_OA_044_tomloc\n",
      "No Valid Runs for SAX_OA_044_tomloc\n",
      "No Valid Runs for SAX_OA_034_tomloc\n",
      "No Valid Runs for SAX_OA_034_tomloc\n",
      "No Valid Runs for SAX_OA_034_tomloc\n",
      "No Valid Runs for SAX_OA_034_tomloc\n",
      "No Valid Runs for SAX_OA_015_tomloc\n",
      "No Valid Runs for SAX_OA_015_tomloc\n",
      "No Valid Runs for SAX_OA_015_tomloc\n",
      "No Valid Runs for SAX_OA_015_tomloc\n",
      "No Valid Runs for SAX_OA_033_tomloc\n",
      "No Valid Runs for SAX_OA_033_tomloc\n",
      "No Valid Runs for SAX_OA_033_tomloc\n",
      "No Valid Runs for SAX_OA_033_tomloc\n",
      "No Valid Runs for SAX_OA_091_tomloc\n",
      "No Valid Runs for SAX_OA_091_tomloc\n",
      "No Valid Runs for SAX_OA_091_tomloc\n",
      "No Valid Runs for SAX_OA_091_tomloc\n",
      "No Valid Runs for SAX_OA_080_tomloc\n",
      "No Valid Runs for SAX_OA_080_tomloc\n",
      "No Valid Runs for SAX_OA_080_tomloc\n",
      "No Valid Runs for SAX_OA_080_tomloc\n",
      "No Valid Runs for SAX_OA_064_tomloc\n",
      "No Valid Runs for SAX_OA_064_tomloc\n",
      "No Valid Runs for SAX_OA_064_tomloc\n",
      "No Valid Runs for SAX_OA_064_tomloc\n",
      "No Valid Runs for SAX_OA_104_tomloc\n",
      "No Valid Runs for SAX_OA_104_tomloc\n",
      "No Valid Runs for SAX_OA_104_tomloc\n",
      "No Valid Runs for SAX_OA_104_tomloc\n",
      "No Valid Runs for SAX_OA_100_tomloc\n",
      "No Valid Runs for SAX_OA_100_tomloc\n",
      "No Valid Runs for SAX_OA_100_tomloc\n",
      "No Valid Runs for SAX_OA_100_tomloc\n",
      "No Valid Runs for SAX_OA_067_tomloc\n",
      "No Valid Runs for SAX_OA_067_tomloc\n",
      "No Valid Runs for SAX_OA_067_tomloc\n",
      "No Valid Runs for SAX_OA_067_tomloc\n",
      "No Valid Runs for SAX_OA_106_tomloc\n",
      "No Valid Runs for SAX_OA_106_tomloc\n",
      "No Valid Runs for SAX_OA_106_tomloc\n",
      "No Valid Runs for SAX_OA_106_tomloc\n",
      "No Valid Runs for SAX_OA_079_tomloc\n",
      "No Valid Runs for SAX_OA_079_tomloc\n",
      "No Valid Runs for SAX_OA_079_tomloc\n",
      "No Valid Runs for SAX_OA_079_tomloc\n",
      "No Valid Runs for SAX_OA_062_tomloc\n",
      "No Valid Runs for SAX_OA_062_tomloc\n",
      "No Valid Runs for SAX_OA_062_tomloc\n",
      "No Valid Runs for SAX_OA_062_tomloc\n",
      "No Valid Runs for SAX_OA_087_tomloc\n",
      "No Valid Runs for SAX_OA_087_tomloc\n",
      "No Valid Runs for SAX_OA_087_tomloc\n",
      "No Valid Runs for SAX_OA_087_tomloc\n",
      "No Valid Runs for SAX_OA_089_tomloc\n",
      "No Valid Runs for SAX_OA_089_tomloc\n",
      "No Valid Runs for SAX_OA_089_tomloc\n",
      "No Valid Runs for SAX_OA_089_tomloc\n"
     ]
    }
   ],
   "source": [
    "taskinfo = pd.read_csv('/om/group/saxelab/OpenAutism/data/Subject_Task_Info_Dima/Description_of_functional_tasks_internal.csv')\n",
    "\n",
    "# now put all data in one big matrix (subject--task)\n",
    "count = -1\n",
    "\n",
    "# load in all tasks in experiments\n",
    "# call function that returns all experiments from runinfo\n",
    "tasks = list_tasks(taskinfo)\n",
    "\n",
    "if pilot == 1:\n",
    "    tasks = ['tomloc'] # just for tomloc\n",
    "else:\n",
    "    tasks = [task for task in tasks if \"tomloc\" not in task ]\n",
    "    print(\"NOTE: Description_of_functional_tasks.csv needs to be edited to exactly match the name of task\")\n",
    "    \n",
    "count = 0\n",
    "counter = 0\n",
    "\n",
    "for subject in subject_list:\n",
    "    \n",
    "    # add task\n",
    "    for task in tasks:\n",
    "    \n",
    "        dfheader = []\n",
    "        counter = counter + 1\n",
    "        imgroot = main_root+'SUBJECTS/'+subject+'/standard/second_level_analyses/'\n",
    "        metric_folders = ['magnitude' ,'lateralization','interregional_cor','temporal_variance']\n",
    "        df0 = pd.DataFrame([[subject, task]],\n",
    "                                  columns=['SAX_ID', 'TASK'])\n",
    "        dflist = [df0]\n",
    "\n",
    "        for metric in metric_folders:\n",
    "            file = ''\n",
    "            if metric is 'magnitude':\n",
    "                file = [subject+'_'+task+'_'+'MAG_SUM_STATS_AVE_RUNS.csv']\n",
    "            elif metric is 'lateralization':\n",
    "                file = [subject+'_'+task+'_'+'LAT_SUM_STATS_AVE_RUNS.csv']\n",
    "            elif metric is 'interregional_cor':\n",
    "                file = [subject+'_'+task+'_'+'INTERREG_SUM_STATS_AVE_RUNS.csv']\n",
    "            elif metric is 'temporal_variance':\n",
    "                file = [subject+'_'+task+'_'+'TEMPNOISE_SUM_STATS_AVE_RUNS.csv']\n",
    "\n",
    "            for ext in range(0,len(file)):\n",
    "                newimgroot = imgroot + metric + '/' + file[ext]\n",
    "\n",
    "                try:\n",
    "                    df = pd.read_csv(newimgroot)\n",
    "                    dflist.append(df[:1])  # add the last row to the list\n",
    "                    #print(df[:1])\n",
    "                    if count <= 1:\n",
    "                        dfheader.append(df.columns.values.tolist())\n",
    "                except FileNotFoundError:\n",
    "                    print(\"No Valid Runs for \"+subject+'_'+task)   \n",
    "        # concatenate list of rows (all dataframes) for one subject\n",
    "        df_concat = pd.concat(dflist, axis=1)\n",
    "        count = count + 1\n",
    "\n",
    "\n",
    "        if counter == 1:\n",
    "            header = [item for sublist in dfheader for item in sublist]\n",
    "            add2header = ['SAX_OA_ID','TASK']\n",
    "            header = add2header+header\n",
    "            df_concat.to_csv(main_root+'/SUMMARY_STATS/ALLSUBJECTS_MATRIX.csv', index=False, mode='w', header=header)\n",
    "        else:\n",
    "            df_concat.to_csv(main_root+'/SUMMARY_STATS/ALLSUBJECTS_MATRIX.csv', index=False, mode='a', header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40a8bcc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40a46ae80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40aa48f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40aaa3518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40a9999b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40ac5a4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40adafdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40ae0a828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40aeea978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40add1a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40b090860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40b2022e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40b048e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40aea69b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba4032daf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40a3a1f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba409373208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba409e81eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba409a45978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40a3c85f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba409e52898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba40a05a898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba4095d8cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba4095e44a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba409f95748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba4095d86d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load in csv as a dataframe\n",
    "if pilot == 1:\n",
    "    temp_root = '/om/group/saxelab/OpenAutism/TIER_OpenAutism/pilot_data/SUMMARY_STATS/ALLSUBJECTS_MATRIX.csv'\n",
    "    #save_dist = '/om/group/saxelab/OpenAutism/TIER_OpenAutism/pilot_data/SUMMARY_STATS/General_Distributions/'\n",
    "    save_dist = '/om/user/rezzo/General_Distributions/'\n",
    "else:\n",
    "    temp_root = '/om/group/saxelab/OpenAutism/TIER_OpenAutism/analysis_data/SUMMARY_STATS/ALLSUBJECTS_MATRIX.csv'\n",
    "    save_dist = '/om/group/saxelab/OpenAutism/TIER_OpenAutism/analysis_data/SUMMARY_STATS/General_Distributions/'\n",
    "\n",
    "df = pd.read_csv(temp_root)\n",
    "\n",
    "df\n",
    "\n",
    "# metric values of interest (for histograms)\n",
    "names = [key for key in dict(df.dtypes) if dict(df.dtypes)[key] in ['float64', 'int64']]\n",
    "\n",
    "# plot the data by metric\n",
    "for element in names:\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    df.hist(element, ax=ax, bins=50)\n",
    "    if element[0:3] == 'LAT':\n",
    "        plt.xlim(-1, 1)\n",
    "    plt.savefig(save_dist+'Metric_Distribution_'+element)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
