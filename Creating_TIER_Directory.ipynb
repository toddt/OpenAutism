{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates new directory with con image, t-maps, z-maps, and temporal BOLD 4D images\n",
    "# and design files for OpenAutism (per subject, per task, per run)\n",
    "#only copy over files on the spreadsheet (/om/user/rezzo/EXPERIMENT_INFO/tomloc_subject_info_internal.csv)\n",
    "\n",
    "# FOR FUTURE / PENDING:\n",
    "# once second level is run, must import one con, t-map and z-map per subject (now importing \n",
    "# one per run)\n",
    "\n",
    "# some subjects are in more than one experiment (e.g. SAX_OA_## in both exp1\n",
    "# and exp2). In these cases, all runs are copied over. There is a second option, by using the function\n",
    "# identify_greater_motion, which returns a list of the old subjectids to be eliminated. Based framewise displacement. \n",
    "\n",
    "pilot = 1        # 1 = pilot_data / 0 = analysis_data\n",
    "\n",
    "# import modules\n",
    "from glob import glob\n",
    "import re\n",
    "import os.path\n",
    "from itertools import repeat\n",
    "import csv\n",
    "from shutil import copyfile\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "if pilot == 1:\n",
    "    analy_dir = 'pilot_data'\n",
    "    conversion = '/om/user/rezzo/TOMLOC_INFO/tomloc_subject_info_internal.csv'\n",
    "    datainfo = pd.read_csv('/om/user/rezzo/TOMLOC_INFO/Data_sets_internal.csv')\n",
    "    details = pd.read_csv('/om/user/rezzo/TOMLOC_INFO/Description_of_functional_tasks_internal.txt', delimiter='\\t')\n",
    "    runinfo = '/om/group/saxelab/OpenAutism/TIER_OpenAutism/pilot_data/RUN_INFO/'\n",
    "else:\n",
    "    analy_dir = 'analysis_data' # note: make sure these csv files do not have special characters (e.g. Ã¤)\n",
    "    conversion = '/om/group/saxelab/OpenAutism/data/Subject_Task_Info_Dima/subject_info_internal.csv'\n",
    "    datainfo = '/om/group/saxelab/OpenAutism/data/Subject_Task_Info_Dima/Data_sets_internal.csv'\n",
    "    details = '/om/group/saxelab/OpenAutism/data/Subject_Task_Info_Dima/Description_of_functional_tasks_external.csv'\n",
    "    runinfo = '/om/group/saxelab/OpenAutism/TIER_OpenAutism/analysis_data/RUN_INFO/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Obtain_OA_list(tmp_root):\n",
    "    all_OA = []\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \",\"):\n",
    "            all_OA.append(line[0])\n",
    "    return all_OA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional function to use, if decide to remove the experiment of duplicate subject based \n",
    "# on greater motion.\n",
    "\n",
    "def identify_greater_motion(subject1, subject2):\n",
    "    tmp_root = '/om/user/rezzo/INCLUDE_final.csv'\n",
    "\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        mot1 = []\n",
    "        mot2 = []\n",
    "        subjectarray = [subject1, subject2]\n",
    "        readCSV = csv.reader(tsv, delimiter='\\t')\n",
    "        for line in readCSV:\n",
    "            if subject1 == line[2]:\n",
    "                mot1.append(float(line[9]))\n",
    "                np.array(mot1)\n",
    "            if subject2 == line[2]:\n",
    "                mot2.append(float(line[9]))\n",
    "                np.array(mot2)\n",
    "    if np.mean(mot1) > np.mean(mot2):\n",
    "        return subject1\n",
    "    elif np.mean(mot2) > np.mean(mot1):\n",
    "        return subject2\n",
    "    else:\n",
    "        return random.choice(subjectarray) # for temporarily missing data, just choose randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_2int(runs):\n",
    "    new_runs = [x.strip() for x in runs.split(', ')]\n",
    "    counter = -1\n",
    "    for el in new_runs:\n",
    "        counter = counter+1\n",
    "        new_runs[counter] = int(el)\n",
    "    return new_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sub(name):\n",
    "    if name[:4] == 'sub-':\n",
    "        name = name[4:]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject look up table conversion (IGNORING undescores)\n",
    "def Convert_Subname(Oldname):\n",
    "    tmp_root = '/om/user/rezzo/Subject_Conversion_Table.csv'\n",
    "\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \",\"):\n",
    "            if Oldname == line[1].replace(\"_\",\"\"):\n",
    "                Newname = line[0]\n",
    "            else:\n",
    "                continue\n",
    "    return Newname  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Underscores(list_of_names):\n",
    "    counter = 0\n",
    "    for el in list_of_names:\n",
    "        list_of_names[counter] = el.replace(\"_\",\"\")\n",
    "        counter = counter + 1\n",
    "    return list_of_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Return_dataset(site, task, details, datainfo):    \n",
    "    dataset = datainfo.loc[datainfo['study_name'] == site, 'data_set'].iloc[0]\n",
    "    dfList = details['data_set'].tolist()\n",
    "    app = []\n",
    "    for idx, el in enumerate(dfList):\n",
    "        numerate = [int(s) for s in el.split(',')]\n",
    "        if int(dataset) in numerate:\n",
    "            if details.get_value(idx,'task_name') == task:\n",
    "                return details.get_value(idx,'TR (sec)'), details.get_value(idx,'condition_1'), details.get_value(idx,'event or block design')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looks for subjects that participated in > 1 experiment\n",
    "\n",
    "df = pd.read_csv(conversion, header=None)\n",
    "\n",
    "# make list of all duplicate items\n",
    "a = df[0].tolist()\n",
    "dup_list = [k for k,v in Counter(a).items() if v>1]\n",
    "\n",
    "repeat_subjects = []\n",
    "remove_subjects = []\n",
    "\n",
    "for el in dup_list:\n",
    "    with open(conversion, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \",\"):\n",
    "            if el == line[0]:\n",
    "                repeat_subjects.append(line[1])\n",
    "\n",
    "list_of_duplicates = [repeat_subjects[i:i + 2] for i in range(0, len(repeat_subjects), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Return_motion(subject, site, run, motioninfo):   \n",
    "def Return_motion(subject, site, task, run, motioninfo):   \n",
    "    res = motioninfo.loc[(motioninfo[1] == site) & (motioninfo[0] == subject) &(motioninfo[3] == task) & (motioninfo[4] == run)]\n",
    "    exclude = int(res[8])\n",
    "    avemotion = float(res[9])\n",
    "    return exclude, avemotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/neuro/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n",
      "/opt/conda/envs/neuro/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs match\n",
      "Runs match\n"
     ]
    }
   ],
   "source": [
    "root = '/om/group/saxelab/OpenAutism/Analysis/first_level_standard.py/'\n",
    "OA_dir = '/om/group/saxelab/OpenAutism/TIER_OpenAutism/'+analy_dir+'/SUBJECTS/'\n",
    "path = '/om/user/rezzo/INCLUDE.csv'\n",
    "motioninfo = pd.read_csv(path, sep='\\t', header=None)\n",
    "\n",
    "df = pd.read_csv(conversion)\n",
    "df\n",
    "dup_list = []\n",
    "UNIQUE_OA_IDS = df['OAID'].unique()\n",
    "UNIQUE_TASKS = df['func_task'].unique()\n",
    "\n",
    "# for each unique OA subject, return row\n",
    "for newname in UNIQUE_OA_IDS:\n",
    "    \n",
    "    if pilot == 1:\n",
    "        UNIQUE_OA_IDS = [ task for task in UNIQUE_OA_IDS if \"tomloc\" in task ]\n",
    "    else:\n",
    "        UNIQUE_OA_IDS = [ task for task in UNIQUE_OA_IDS if \"tomloc\" not in task ]\n",
    "\n",
    "        #create OA directory in TIER folder is non-existent\n",
    "    if (not os.path.exists(OA_dir+newname)):\n",
    "\n",
    "        proc_streams = ['first_level_standard.py','first_level_aroma.py','first_level_gorgolewski.py']\n",
    "        time_streams = ['first_level_standard_timeseries.py','first_level_aroma_timeseries.py','first_level_gorgolewski_timeseries.py']\n",
    "\n",
    "        for stream in range(0, len(proc_streams)):\n",
    "\n",
    "            stream_abrv = find_between(proc_streams[stream], \"level_\", \".py\" )\n",
    "\n",
    "            # make directories\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'first_level_analyses','BOLD_data')) \n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','multivariate'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','lateralization'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','magnitude'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','interregional_cor'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','temporal_variance'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','MISC','individual_roi_masks'))\n",
    "            os.makedirs(os.path.join(OA_dir+newname,stream_abrv,'second_level_analyses','MISC','mean_roi_Temporal_Signal'))\n",
    "\n",
    "            for task in UNIQUE_TASKS:\n",
    "                # create unique rows\n",
    "                dup_list = []\n",
    "                dup_list.append(df.index[(df['OAID'] == newname) & (df['func_task'] == task)].tolist())\n",
    "                task = Remove_Underscores([task])\n",
    "                task = task[0]\n",
    "                run_counter = 0\n",
    "                \n",
    "                for items in dup_list: # each item is the indices list of all runs for a task (across experiments)\n",
    "                    site = df.iloc[items]['data_set'].astype(str)\n",
    "                    site = site.tolist() #site\n",
    "                    found = df.iloc[items]['SUBJID'].astype(str)\n",
    "                    found = Remove_Underscores(found.tolist()) #subjid\n",
    "                    \n",
    "                    for el in range(0,len(items)): \n",
    "                        items_index = items[el]\n",
    "                        runs = df.iloc[items_index]['BIDS_runs']\n",
    "                        runs = Convert_2int(runs)\n",
    "                        \n",
    "                        for run in runs:\n",
    "                            \n",
    "                            run_counter = run_counter+1\n",
    "                            \n",
    "                            #added\n",
    "                            if stream == 0:\n",
    "                                #SAVE MOTION INTO\n",
    "                                [exclude, avemotion] = Return_motion(newname,site[el], task, run, motioninfo)\n",
    "                                \n",
    "                                # SAVE INFO NEEDED IN METRIC SCRIPT (needed b/c subjects in multiple experiments)\n",
    "                                [TR, COND, BLOCK] = Return_dataset(site[el], task, details, datainfo)\n",
    "                                with open(runinfo+\"RUN_INFO.txt\",'a') as fd:\n",
    "                                    fd.write(newname + \"\\t\" + site[el] + \"\\t\" + task + \"\\t\" + str(run_counter) + \"\\t\" + str(TR) + \"\\t\" + str(COND) + \"\\t\" + str(BLOCK) +\"\\t\" +str(exclude)+\"\\t\" +str(avemotion) +\"\\n\")\n",
    "                                #fd.close()\n",
    "                                \n",
    "                                # look for any runs that have to be manually deleted due to scanner issue\n",
    "                                originalbidruns = '/om/group/saxelab/OpenAutism/data/'+site[el]+'/BIDS/'+'sub-'+found[el]+'/func/'\n",
    "                                bidruns = [filename for filename in sorted(os.listdir(originalbidruns)) if (filename.startswith('sub-'+found[el]) and filename.endswith(\".nii.gz\"))]\n",
    "                                amount = len(bidruns)\n",
    "                                if amount != len(runs):\n",
    "                                    print(found[el]': BID funcs > valid runs. May reqcuire manual deletion.')\n",
    "                                \n",
    "                            src_dir = '/om/group/saxelab/OpenAutism/Analysis/'+proc_streams[stream]+'/'+site[el]+'/sub-'+found[el]+'/'+task+'/model/run'+str(run)+'/'\n",
    "                            design_dir = '/om/group/saxelab/OpenAutism/data/TextFiles/tsv_text_files/'+site[el]+'/'\n",
    "                            raw_dir = \"/om/group/saxelab/OpenAutism/Analysis/\"+time_streams[stream]+'/'+site[el]+\"/sub-\"+found[el]+'/'+task+\"/model/run\"+str(run)+'/'\n",
    "                            \n",
    "                            # if the run is not a motion outlier:\n",
    "                            \n",
    "                            try:\n",
    "                                #con, tmap, zmap\n",
    "                                prefixed1 = [filename for filename in sorted(os.listdir(src_dir)) if (filename.startswith(\"con_1_\") and filename.endswith(\"_tstat.nii.gz\"))]\n",
    "                                copyfile(src_dir+prefixed1[0], OA_dir+newname+'/'+stream_abrv+'/first_level_analyses/tstat1_'+task+'_run-00'+str(run_counter)+'.nii.gz')\n",
    "                                prefixed2 = [filename for filename in sorted(os.listdir(src_dir)) if (filename.startswith(\"con_1_\") and filename.endswith(\"_cope.nii.gz\"))]\n",
    "                                copyfile(src_dir+prefixed2[0], OA_dir+newname+'/'+stream_abrv+'/first_level_analyses/cope1_'+task+'_run-00'+str(run_counter)+'.nii.gz')\n",
    "                                prefixed3 = [filename for filename in sorted(os.listdir(src_dir)) if (filename.startswith(\"con_1_\") and filename.endswith(\"_zstat.nii.gz\"))]\n",
    "                                copyfile(src_dir+prefixed3[0], OA_dir+newname+'/'+stream_abrv+'/first_level_analyses/zstat1_'+task+'_run-00'+str(run_counter)+'.nii.gz')\n",
    "                            except FileNotFoundError:\n",
    "                                print('WARNING: missing image file(s) in '+src_dir)\n",
    "                                \n",
    "                            try:\n",
    "                                #temporal runs\n",
    "                                prefixed4 = [filename for filename in sorted(os.listdir(raw_dir)) if (filename.startswith(\"res4d\") and filename.endswith(\".nii.gz\"))]\n",
    "                                copyfile(raw_dir+prefixed4[0], OA_dir+newname+'/'+stream_abrv+'/first_level_analyses/BOLD_data/temporaldata_'+task+'_run-00'+str(run_counter)+'.nii.gz')\n",
    "                            except FileNotFoundError:\n",
    "                                print('WARNING: missing temopral file(s) in '+raw_dir)\n",
    "                                \n",
    "                            try:\n",
    "                                #design file\n",
    "                                copyfile(design_dir+'sub-'+found[el]+'_task-'+task+'_run-00'+str(run)+'_events.tsv', OA_dir+newname+'/'+stream_abrv+'/first_level_analyses/BOLD_data/design_'+task+'_run-00'+str(run_counter)+'.tsv') \n",
    "                            except FileNotFoundError:\n",
    "                                print('WARNING: missing design file(s) in '+design_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in file with all run info and make a file listing only runs to include\n",
    "\n",
    "if pilot == 1:\n",
    "    con = 'pilot_data'\n",
    "else:\n",
    "    con = 'analysis_data'\n",
    "    \n",
    "runinfo = '/om/group/saxelab/OpenAutism/TIER_OpenAutism/'+con+'/RUN_INFO/RUN_INFO.txt'\n",
    "\n",
    "Motion_filt_lenient = []\n",
    "\n",
    "with open(runinfo, \"r\") as infile:\n",
    "    X = [list(map(str, line.split())) for line in infile]\n",
    "    for line in X:\n",
    "        if line[7] == '0':\n",
    "            Motion_filt_lenient.append(line)\n",
    "        \n",
    "# save to a text file\n",
    "fo = open(\"/om/group/saxelab/OpenAutism/TIER_OpenAutism/\"+con+\"/RUN_INFO/Runs_LenientMotionFiltered.tsv\", \"w+\")\n",
    "for element in range (0,len(Motion_filt_lenient)):\n",
    "    fo.write(Motion_filt_lenient[element][0]+ Motion_filt_lenient[element][2]+'00'+Motion_filt_lenient[element][3]+ \"\\n\")\n",
    "fo.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
