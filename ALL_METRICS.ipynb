{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERAL ROI INFORMATION:\n",
    "# This script uses functional roi niftis (converted from.img files and scaled in FSL to correct\n",
    "# size in each dimension). These parcels are used as binary masks on output from first-level \n",
    "# analysis. \n",
    "# This yields a hypothesis space for each roi (same location for every \n",
    "# subject). These ROI's are face parcels from the Kanwisher website http://web.mit.edu/bcs/nklab/GSS.shtml\n",
    "# with the exception of DMFPC and AMY (taken from saxelab).\n",
    "\n",
    "## MAGNITUDE\n",
    "# The top N (default = 50) voxels are then selected from these hypothesis space based on highest t-statistic\n",
    "# (no threshold, and no constraint on contiguity). This defines the ROIs per subject.\n",
    "# The corresponding con values are averaged as the mean magnitude summary statistic.\n",
    "# Also computes average position\n",
    "\n",
    "## LATERALIZATION\n",
    "# Different MASKS used. Masks created by summing hypothesis space and flipped opposite space, \n",
    "# e.g. rTPJ computed as voxels within rTPJ parcel and flipped lTPJ. See commands:\n",
    "# TO FLIP: fslswapdim data -x y z flip_data\n",
    "# TO ADD: fslmaths mask1 -add mask2 -bin output ... (e.g. mask1 = rTPJ original, mask 2 = lTPJ flipped)\n",
    "# Count NUMBER of voxels above a certain p value (0.01 or 0.001) to do calculations: L - R / L + R\n",
    "\n",
    "## INTERREGIONAL CORRELATIONS\n",
    "# Takes mean temporal signal in each roi and creates a pearson's r correlation matrix. Converts \n",
    "# this to Fisherman's Z, and then averages for a summary statistic. Does this per run, and saves\n",
    "# both run data and averaged run data as separate csv files.\n",
    "\n",
    "## TEMPORAL NOISE\n",
    "# Details in section. Looks at specified number of points relative to hrf, and uses these\n",
    "# indices to extract N BOLD values for each trial; finds std for each trial, then averages \n",
    "# across face conditions. Does this for each run, saves runs separately and the mean summary in csv.\n",
    "\n",
    "## MULTIVARIATE VECTORS\n",
    "# This is computed over entire hypothesis space, as a vector of all contrast values within the parcel.\n",
    "\n",
    "# SAVES: \n",
    "# roi_individual_masks & roi_contrast_values (top N voxels per subject), \n",
    "# and text files per metric computations that can be opened as a dataframe.\n",
    "\n",
    "# MASKS DIRECTORY\n",
    "# /om/user/rezzo/OpenAutism/analysis_data/MASKS/STANDARD_MASKS\n",
    "# /om/user/rezzo/OpenAutism/analysis_data/MASKS/LATERAL_MASKS\n",
    "\n",
    "# Note : to convert Analyze format to Nifti format:\n",
    "# load in header, image (and .mat) files into the folder\n",
    "# fslchfiletype NIFTI_GZ RTPJ_xyz.nii.gz RTPJ_xyz\n",
    "\n",
    "# SETTINGS\n",
    "pilot = 1           # pilot == 1 runs on tomloc; 0 runs on analysis_data\n",
    "roi_size = 50       # default number of voxels to define individual roi == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from os.path import join as pjoin, split as psplit\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from statistics import mean\n",
    "import csv\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import warnings as warn\n",
    "import operator\n",
    "import itertools\n",
    "from nipype.interfaces import fsl\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "import nilearn\n",
    "from nilearn.masking import apply_mask\n",
    "from nilearn import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject look up table conversion (IGNORING undescores)\n",
    "def Convert_Subname(Newname):\n",
    "    tmp_root = '/om/user/rezzo/Subject_Conversion_Table.csv'\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \",\"):\n",
    "            if Newname == line[0]:\n",
    "                Oldname = line[1]\n",
    "            else:\n",
    "                continue\n",
    "    return Oldname  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain values and indices of individual rois\n",
    "def nan_largestval(ary, n):\n",
    "    flat = ary.flatten()                      # transform to one array\n",
    "    values = -np.sort(-flat)                  # order values greatest to least, nans at the end\n",
    "    idx = (-flat).argsort()[:n]               # obtain indices of the values in flat array\n",
    "    idx2 = np.unravel_index(idx, ary.shape)   # transform indices to original array\n",
    "    return [values[0:n], idx2]                # return values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain the count for the lateralization calculations\n",
    "def nan_abovethresh(ary, thresh):\n",
    "    flat = ary.flatten()                     # transform to one array\n",
    "    count = np.sum(flat<thresh)              # count voxels greater than thresh\n",
    "    return [count]                           # return count                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to return a list of floats from a text file  \n",
    "def txt2list(file,details):\n",
    "    txtfile = open(file,'r')\n",
    "    details = []\n",
    "    for line in txtfile:\n",
    "        details.extend([float(i) for i in line.rstrip('\\n').split()])\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to transform pearson's R to fisher's Z\n",
    "def pearson2fisher(pearsonR):\n",
    "    fisherZ = 0.5*(np.log(1+pearsonR) - np.log(1- pearsonR)) # np.log is NATURAL LOG!*\n",
    "    return fisherZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to cut a substring out of a larger string\n",
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find between but with list input\n",
    "def find_list(lis, str1, str2):\n",
    "    init = []\n",
    "    for el in lis:\n",
    "        init.append(find_between(el, str1,str2))\n",
    "        norepeat = list(set(init))\n",
    "        final = list(filter(None, norepeat)) # fastest\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out first element of design file\n",
    "def Design_file(tmp_root):\n",
    "    array = []\n",
    "    with open(tmp_root, \"r\") as tsv:\n",
    "        for line in csv.reader(tsv,  delimiter = \"\\t\"):\n",
    "            array.append(line[3])\n",
    "    if array[1] == 'belief':\n",
    "        return \"1\"\n",
    "    elif array[1] == 'photo':\n",
    "        return \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files in directory and obtain a list of the file names\n",
    "def roi_list(directory_name):\n",
    "    full_list = os.listdir(directory_name)\n",
    "    for element in range(0, len(full_list)):\n",
    "        #replace = re.match(\"(.*?)_\",full_list[element]).group()\n",
    "        replace = full_list[element].split('_')[0]\n",
    "        full_list[element] = replace\n",
    "    return full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervals(start_num, N):\n",
    "    init = []\n",
    "    counter = -1\n",
    "    for el in range(0, N):\n",
    "        counter = counter + 1\n",
    "        init.append(start_num + counter)\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create the appropriate list of lateral rois\n",
    "def lat_roi_condense(roi_list):\n",
    "    for elements in range(0, len(roi_list)):\n",
    "        roi_list[elements] = roi_list[elements][1:]\n",
    "    return list(set(roi_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create roi_indexes, and subject indexes\n",
    "\n",
    "if pilot is 1:\n",
    "    main_root = '/om/user/rezzo/OpenAutism/pilot_data/'\n",
    "else:\n",
    "    main_root = '/om/user/rezzo/OpenAutism/analysis_data/'\n",
    "\n",
    "roiroot = main_root+'MASKS/STANDARD_MASKS/'\n",
    "biroiroot = main_root+'MASKS/LATERAL_MASKS/'\n",
    "roi_index = roi_list(roiroot)\n",
    "lat_index = lat_roi_condense(roi_list(biroiroot))\n",
    "#lat_index = lat_roi_condense(lat_index)\n",
    "    \n",
    "all_subjects = glob(main_root+\"/SUBJECTS/*/\")\n",
    "subject_list = []\n",
    "\n",
    "# create list of subject in folder\n",
    "for subs in range(0, len(all_subjects)):\n",
    "    m = re.search('SAX_OA_(.+?)/', all_subjects[subs])\n",
    "    if m:\n",
    "        found = m.group(1)\n",
    "        subject_list.append('SAX_OA_'+found) # subject is the list with all subject names.\n",
    "\n",
    "# Here is the sorted list of OA subject names        \n",
    "subject_list.sort\n",
    "\n",
    "# load list of subjects to include after motion outlier exclusion\n",
    "good_subjects = main_root+'MOTION_INFO/Runs_LenientMotionFiltered.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/neuro/lib/python3.6/site-packages/ipykernel_launcher.py:72: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAX_OA_001_tomloc002\n",
      "SAX_OA_001_tomloc001\n"
     ]
    }
   ],
   "source": [
    "######## MAGNITUDE AND LATERALIZATION ##########\n",
    "# NOTE: this section identifies and excludes motion outliers and performs run by run\n",
    "# this is redundant if these measures have been priorly performed but will not affect results\n",
    "\n",
    "subject_list = ['SAX_OA_001'] #,'SAX_OA_006','SAX_OA_076', 'SAX_OA_096']\n",
    "\n",
    "for subject in subject_list:\n",
    "    warn.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
    "    imgroot = main_root+'SUBJECTS/'+subject+'/standard/'\n",
    "    temp_list = os.listdir(imgroot+'first_level_analyses')\n",
    "    \n",
    "    # identify number of tasks for this subject\n",
    "    task_list = find_list(temp_list, \"tstat1_\", '_run-')  \n",
    "    \n",
    "    #identify number of runs for this task\n",
    "    for task in task_list:\n",
    "        runs = find_list(temp_list, \"tstat1_\"+task+'_run-', '.nii.gz') \n",
    "        iteration = 0\n",
    "        \n",
    "        # for each run in current task\n",
    "        for run in runs:\n",
    "            # check if it is a motion outlier:\n",
    "            if subject+task+run in open(good_subjects).read():\n",
    "                iteration = iteration + 1\n",
    "                T_image = os.path.join(imgroot+'first_level_analyses/'+'tstat1_'+task+'_run-'+run+'.nii.gz')\n",
    "                CON_image = os.path.join(imgroot+'first_level_analyses/'+'cope1_'+task+'_run-'+run+'.nii.gz')\n",
    "                Z_image = os.path.join(imgroot+'first_level_analyses/zstat1_'+task+'_run-'+run+'.nii.gz')\n",
    "\n",
    "                # load t-image, z-image, con-image\n",
    "                t_image = nib.load(T_image)\n",
    "                z_image = nib.load(Z_image)\n",
    "                con_image = nib.load(CON_image)\n",
    "\n",
    "                # convert images to numpy arrays\n",
    "                t_data = np.array(t_image.dataobj)\n",
    "                z_data = np.array(z_image.dataobj)\n",
    "                CON_data = np.array(con_image.dataobj)\n",
    "\n",
    "                # initialize lists\n",
    "                mag_per_roi = [[]] * len(roi_index) # ave magnitude\n",
    "                pos_per_roi = [[]] * len(roi_index) # ave position\n",
    "                lat_lenient = [[]] * len(lat_index) # lat for low thresh\n",
    "                count_lenient = [[]] * len(lat_index) # count of total voxels above low thresh\n",
    "                lat_strict = [[]] * len(lat_index)  # lat for high thresh\n",
    "                count_strict = [[]] * len(lat_index) # count of total voxels above high thresh\n",
    "\n",
    "                count = -1\n",
    "\n",
    "                for roi in roi_index:\n",
    "                    count = count + 1\n",
    "                    parcel_file = os.path.join(data_path, roiroot + roi+ '_FSL_space.nii.gz')\n",
    "                    PARCEL = nib.load(parcel_file)\n",
    "                    binary_data = np.array(PARCEL.dataobj) #this is the mask\n",
    "\n",
    "                    # make all values in binary mask = 0 to NAN\n",
    "                    binary_data[binary_data == 0] = 'nan'\n",
    "\n",
    "                    # save as a flattened array\n",
    "                    roi_con = CON_data*binary_data #just added this\n",
    "                    MVPA_array = roi_con.flatten()\n",
    "                    np.save(imgroot+'second_level_analyses/multivariate/MVPA_array', MVPA_array)\n",
    "\n",
    "                    # multiply t-image by roi masks;\n",
    "                    roi_hs = t_data*binary_data\n",
    "                    roi_hs = roi_hs.astype('float')\n",
    "\n",
    "                    # find top N t-values within this space to define individual's ROI\n",
    "                    [values, indices] = nan_largestval(roi_hs, roi_size)\n",
    "\n",
    "                    # initialize individual mask space\n",
    "                    roi_mask = np.zeros(shape=roi_hs.shape)\n",
    "                    binary_data[binary_data == 'nan'] = 0   # temporary, for saving mask\n",
    "\n",
    "                    # create subject-specific roi mask (N voxels, e.g. 50)\n",
    "                    for hh in range(roi_size):\n",
    "                        roi_mask[indices[0][hh],indices[1][hh],indices[2][hh]] = 1\n",
    "\n",
    "                    # save it as nifti image\n",
    "                    roi_img = nib.Nifti1Image(roi_mask, PARCEL.affine, PARCEL.header)\n",
    "                    fname = pjoin(imgroot+'second_level_analyses/MISC/individual_roi_masks/'+roi+'_indiv_roi_mask_run'+run)\n",
    "                    nib.save(roi_img, fname)\n",
    "\n",
    "                    # calculate average position for subject's run\n",
    "                    X = int(np.mean(indices[0]))\n",
    "                    Y = int(np.mean(indices[1]))\n",
    "                    Z = int(np.mean(indices[2]))\n",
    "                    location = [X, Y, Z]\n",
    "                    pos_per_roi[count] = location      \n",
    "\n",
    "                    #multiply roi_mask defined by highest t-vaues with con image to obtain magnitude of contrast\n",
    "                    roi_mask[roi_mask == 0] = 'nan'\n",
    "                    roi_contrast = CON_data*roi_mask\n",
    "                    means = np.nanmean(roi_contrast)\n",
    "                    mag_per_roi[count] = means # summary stat\n",
    "\n",
    "                    # Save con values selected by the mask as a flat array\n",
    "                    Topvoxels = roi_contrast.flatten()\n",
    "                    Topvoxels = Topvoxels[~np.isnan(Topvoxels)]\n",
    "                    np.save(imgroot+'second_level_analyses/magnitude/'+subject+task+run+'_Top'+str(roi_size)+'voxels_contrast_'+roi, Topvoxels)\n",
    "\n",
    "                ############################# LATERALIZATION ###############################\n",
    "\n",
    "                count = -1\n",
    "\n",
    "                for roi in lat_index:\n",
    "                    count = count + 1\n",
    "\n",
    "                    combined_left = os.path.join(data_path, biroiroot + 'l'+roi+'_lateral_FSL_space.nii.gz')\n",
    "                    combined_right = os.path.join(data_path, biroiroot + 'r'+roi+'_lateral_FSL_space.nii.gz') \n",
    "\n",
    "                    # load parcels\n",
    "                    LEFT_PARCEL = nib.load(combined_left)\n",
    "                    RIGHT_PARCEL = nib.load(combined_right)\n",
    "\n",
    "                    # make into numpy arrays\n",
    "                    left_binary_data = np.array(LEFT_PARCEL.dataobj)\n",
    "                    right_binary_data = np.array(RIGHT_PARCEL.dataobj)\n",
    "\n",
    "                    # make all values in binary mask = 0 to NAN (esp. needed for python 2)\n",
    "                    left_binary_data[left_binary_data == 0] = 'nan'\n",
    "                    right_binary_data[right_binary_data == 0] = 'nan'\n",
    "\n",
    "                    p_values_1 = stats.norm.sf(abs(z_data))   #one-sided\n",
    "                    p_values_2 = stats.norm.sf(abs(z_data))*2 #twosided (using this one)\n",
    "\n",
    "                    #multiply p-values by roi masks;\n",
    "                    left_roi_hs = (p_values_2)*left_binary_data\n",
    "                    right_roi_hs = (p_values_2)*right_binary_data\n",
    "\n",
    "                    #convert to float\n",
    "                    left_roi_hs = left_roi_hs.astype('float')\n",
    "                    right_roi_hs = right_roi_hs.astype('float')                  \n",
    "\n",
    "                    thresh1 = 0.01\n",
    "                    thresh2 = 0.001\n",
    "\n",
    "                    ## thresh1 calculations\n",
    "                    #count number of voxels on the left that are p < 0.01\n",
    "                    [total_left1] = nan_abovethresh(left_roi_hs, thresh1)\n",
    "\n",
    "                    #count number of voxels on the right that are p < 0.01\n",
    "                    [total_right1] = nan_abovethresh(right_roi_hs, thresh1)\n",
    "\n",
    "                    count_lenient[count] = total_left1 + total_right1\n",
    "\n",
    "                    ## thresh2 calculations\n",
    "                    #count number of voxels on the left that are p < 0.001\n",
    "                    [total_left2] = nan_abovethresh(left_roi_hs, thresh2)\n",
    "\n",
    "                    #count number of voxels on the right that are p < 0.001\n",
    "                    [total_right2] = nan_abovethresh(right_roi_hs, thresh2)\n",
    "\n",
    "                    count_strict[count] = total_left2 + total_right2\n",
    "\n",
    "                    ## final calculations of lateralization\n",
    "                    # -1 means right dominant and 1 means left dominant, 0 means bilaterial\n",
    "\n",
    "                    # for p < 0.01\n",
    "                    if float(total_left1 + total_right1) == 0:\n",
    "                        lat_lenient[count] = 0.0\n",
    "                    else:\n",
    "                        Lat1 = float(total_left1 - total_right1) / float(total_left1 + total_right1)\n",
    "                        lat_lenient[count] = Lat1\n",
    "\n",
    "                    # for p < 0.001\n",
    "                    if float(total_left2 + total_right2) == 0:\n",
    "                        lat_strict[count] = 0.0\n",
    "                    else:\n",
    "                        Lat2 = float(total_left2 - total_right2) / float(total_left2 + total_right2)\n",
    "                        lat_strict[count] = Lat2\n",
    "\n",
    "                ################## Create the dataframes for all metrics ###############              \n",
    "\n",
    "                pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "                # dataframe for magnitude\n",
    "                mag_init = mag_per_roi\n",
    "                mag_init.extend(['Mean contrast magnitude of the top 50 voxels by t-value'])\n",
    "                Mdataframe_init = ['MAG_' + s for s in roi_index]\n",
    "                Mdataframe_init.extend(['Description'])\n",
    "                Msub_measures = pd.DataFrame([mag_init],\n",
    "                                           columns = Mdataframe_init)\n",
    "                Msub_measures.rename(index={0: 'Magnitude'})\n",
    "\n",
    "                # dataframe for position\n",
    "                pos_new = pos_per_roi\n",
    "                pos_new.extend(['XYZ average position of the top voxels of individual roi based on top t-values'])\n",
    "                POS_measures = pd.DataFrame([pos_new],\n",
    "                                columns = Mdataframe_init)\n",
    "                POS_measures.rename(index={0: 'Position'})\n",
    "\n",
    "                # dataframe for both lateralization indices\n",
    "                lat_init = lat_lenient\n",
    "                lat_init2 = lat_strict\n",
    "                lat_init.extend(['Lateralization index based on voxel selection with p < 0.01 (left count - right count / left and right count)'])\n",
    "                lat_init2.extend(['Lateralization index based on voxel selection with p < 0.001 (left count - right count / left and right count)'])\n",
    "                Ldataframe_init = ['LAT_' + s for s in lat_index]\n",
    "                Ldataframe_init.extend(['Description'])\n",
    "                Lsub_measures = pd.DataFrame([lat_init],\n",
    "                                               columns = Ldataframe_init)\n",
    "                Lsub_measures2 = pd.DataFrame([lat_init2],\n",
    "                                               columns = Ldataframe_init)\n",
    "                Lsub_measures.rename(index={0: 'Laterality Index (p < 0.01)'})\n",
    "                Lsub_measures2.rename(index={0: 'Laterality Index (p < 0.001)'})\n",
    "\n",
    "                ## dataframe for both lateralization counts (total voxels above thresh)\n",
    "                count_init = count_lenient\n",
    "                count_init.extend(['Total count based on voxel selection with p < 0.01 (left + right)'])\n",
    "                count_init2 = count_strict\n",
    "                count_init2.extend(['Total count based on voxel selection with p < 0.001 (left + right)'])\n",
    "                count_measures1 = pd.DataFrame([count_init],\n",
    "                                               columns = Ldataframe_init)\n",
    "                count_measures2 = pd.DataFrame([count_init2],\n",
    "                                               columns = Ldataframe_init)\n",
    "\n",
    "                # save (or append) all dataframes to csv file\n",
    "                if iteration == 1:\n",
    "                    Msub_measures.to_csv(imgroot+'second_level_analyses/magnitude/'+subject+'_'+task+'_MAG_SUM_STATS.csv', mode='w', index=False)\n",
    "                    Lsub_measures.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_SUM_STATS.csv', mode='w', index=False)\n",
    "                    Lsub_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_SUM_STATS.csv', mode='w', index=False)\n",
    "                    count_measures1.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_COUNT_STATS.csv', mode='w', index=False)\n",
    "                    count_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_COUNT_STATS.csv', mode='w', index=False)\n",
    "                    POS_measures.to_csv(imgroot+'second_level_analyses/MISC/'+subject+'_'+task+'_POSITION_ROI_STATS.csv', mode = 'w', index=False)\n",
    "                else:\n",
    "                    Msub_measures.to_csv(imgroot+'second_level_analyses/magnitude/'+subject+'_'+task+'_MAG_SUM_STATS.csv', mode='a', index=False, header=False)\n",
    "                    Lsub_measures.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_SUM_STATS.csv', mode='a', index=False, header=False)\n",
    "                    Lsub_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_SUM_STATS.csv', mode='a', index=False, header=False)\n",
    "                    count_measures1.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_COUNT_STATS.csv', mode='a', index=False, header=False)\n",
    "                    count_measures2.to_csv(imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_COUNT_STATS.csv', mode='a', index=False, header=False)\n",
    "                    POS_measures.to_csv(imgroot+'second_level_analyses/MISC/'+subject+'_'+task+'_POSITION_ROI_STATS.csv', mode='a', index=False, header=False)\n",
    "\n",
    "                # log each completed run on console\n",
    "                print(subject+'_'+task+run)\n",
    "                \n",
    "            else:\n",
    "                print(subject+'_'+task+run+' excluded due to motion')\n",
    "        #####################################################################\n",
    "\n",
    "        # make a list of all the files (these files include each run)\n",
    "        file_list = [imgroot+'second_level_analyses/magnitude/'+subject+'_'+task+'_MAG_SUM_STATS.csv', imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_SUM_STATS.csv',\n",
    "                     imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_SUM_STATS.csv', imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT_COUNT_STATS.csv',\n",
    "                    imgroot+'second_level_analyses/lateralization/'+subject+'_'+task+'_LAT2_COUNT_STATS.csv']\n",
    "\n",
    "        # now average all metrics runs together, print exception for tasks with no valid runs\n",
    "        for file in file_list:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                ex = pd.DataFrame(df.mean())\n",
    "                ex = ex.transpose()\n",
    "                ex.to_csv(file[:-4]+'_AVE_RUNS.csv', mode='w', index=False)\n",
    "            except FileNotFoundError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAX_OA_006tomloc001 is a motion outlier\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/om/user/rezzo/OpenAutism/pilot_data/SUBJECTS/SAX_OA_098/standard/first_level_analyses/BOLD_data/design_tomloc_run-002.tsv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ac2c2498e956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# read in design file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgroot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'first_level_analyses/BOLD_data/design_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_run-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mdesignfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m# these lists will separate each face condition within a run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/neuro/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/neuro/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/neuro/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/neuro/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/neuro/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/om/user/rezzo/OpenAutism/pilot_data/SUBJECTS/SAX_OA_098/standard/first_level_analyses/BOLD_data/design_tomloc_run-002.tsv' does not exist"
     ]
    }
   ],
   "source": [
    "############# INTERREGIONAL CORRELATION AND TEMPORAL VARIANCE ###############\n",
    "\n",
    "#subject_list = ['SAX_OA_006']\n",
    "\n",
    "# define the experimental conditions for all experiments\n",
    "if pilot == 1:\n",
    "    faces = ['belief'] # dummy face for tomloc\n",
    "    TR = 2\n",
    "else:\n",
    "    faces = [] # need a list of all analysis_data face conditions\n",
    "\n",
    "# define N discrete points for temporal variance (same place relative to hrf across trials)\n",
    "DISCRETE_POINTS = 4\n",
    "\n",
    "for subject in subject_list:\n",
    "    warn.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
    "    imgroot = main_root+'SUBJECTS/'+subject+'/standard/'\n",
    "    temp_list = os.listdir(imgroot+'first_level_analyses/BOLD_data')\n",
    "    \n",
    "    # identify number of tasks for this subject\n",
    "    task_list = find_list(temp_list, \"temporaldata_\", '_run-')  \n",
    "    \n",
    "    #identify number of runs for this task\n",
    "    for task in task_list:\n",
    "        runs = find_list(temp_list, \"temporaldata_\"+task+'_run-', '.nii.gz') \n",
    "        iteration = 0\n",
    "\n",
    "        # need to determine if block or event-related for analysis_data\n",
    "\n",
    "        # need to figure out TR for analysis_data\n",
    "\n",
    "        # for each run in current task\n",
    "        for run in runs:\n",
    "\n",
    "            Ztrans_values = []\n",
    "\n",
    "            # check if it is a motion outlier:\n",
    "            if subject+task+run in open(good_subjects).read():\n",
    "                iteration = iteration + 1\n",
    "                ######################### TEMPORAL VARIANCE ######################\n",
    "                # read in design file\n",
    "                fpath = imgroot+'first_level_analyses/BOLD_data/design_'+task+'_run-'+run+'.tsv'\n",
    "                designfile = pd.read_csv(fpath, sep='\\t')\n",
    "\n",
    "                # these lists will separate each face condition within a run\n",
    "                onset_list = [[]*len(faces)]\n",
    "                shifted_onsets = [[]*len(faces)]\n",
    "                duration_list = [[]*len(faces)]\n",
    "                count = -1\n",
    "\n",
    "                for element in faces:\n",
    "                    # create list specific to each face condition within a trial\n",
    "                    count = count + 1\n",
    "                    onset_list[count] = designfile.loc[designfile[designfile['trial_type'] == element].index.tolist(),'onset']\n",
    "                    onset_list[count] = [ np.round(x) for x in onset_list[count]] \n",
    "                    duration_list[count] = designfile.loc[designfile[designfile['trial_type'] == element].index.tolist(),'duration']\n",
    "                    # the TR index\n",
    "                    shifted_onsets[count] = (designfile.loc[designfile[designfile['trial_type'] == element].index.tolist(),'onset'] + TR) / TR\n",
    "                    shifted_onsets[count] = [ np.round(x) for x in shifted_onsets[count]] # offset rounded.\n",
    "                    shifted_onsets[count] = [ int(x) for x in shifted_onsets[count]]\n",
    "                    # flatten all lists (treating all face conditions as one condition)\n",
    "                    onset_list = [item for sublist in onset_list for item in sublist]\n",
    "                    duration_list = [item for sublist in duration_list for item in sublist]\n",
    "                    shifted_onsets = [item for sublist in shifted_onsets for item in sublist]\n",
    "\n",
    "                    n_items = len(shifted_onsets) # N face conditions in this run\n",
    "\n",
    "                final_exp_points = [[]]*n_items # initialize list for N arrays\n",
    "\n",
    "                # this array will contain subarrays (temporal indices)\n",
    "                for element in range(0, n_items):\n",
    "                    temp =[]\n",
    "                    temp = intervals(shifted_onsets[element], DISCRETE_POINTS)\n",
    "                    final_exp_points[element]= temp\n",
    "\n",
    "                # load in 4D functional data of current run\n",
    "                Time_image = os.path.join(imgroot+'first_level_analyses/BOLD_data/temporaldata_'+task+'_run-'+run+'.nii.gz')\n",
    "                time_image = nib.load(Time_image)\n",
    "                time_data = np.array(time_image.dataobj)\n",
    "\n",
    "                # initialize arrays related to roi\n",
    "                roicount = -1\n",
    "                temporal_arrays = [[]] * len(roi_index) # will contain mean temporal array of roi\n",
    "                TEMPORAL_VAR = [] # variable of interest (final temporal variance for this run)\n",
    "\n",
    "                for roi in roi_index:\n",
    "                    roicount = roicount + 1\n",
    "                    test = []\n",
    "                    # initializing\n",
    "                    final_exp_values = [[]]*len(final_exp_points)\n",
    "                    std_exp_values = [[]]*len(final_exp_points)\n",
    "                    std_mean_values = [[]]*len(final_exp_points)\n",
    "\n",
    "                    # load in individual roi mask\n",
    "                    roi_file = os.path.join(data_path, imgroot+'second_level_analyses/MISC/individual_roi_masks/'+roi+'_indiv_roi_mask_run'+run+'.nii')\n",
    "                    ROI = nib.load(roi_file)\n",
    "                    binary_data = np.array(ROI.dataobj)\n",
    "\n",
    "                    # use fsl to extract mean signal in roi: \n",
    "                    meants = fsl.ImageMeants(in_file=imgroot+'first_level_analyses/BOLD_data/temporaldata_'+task+'_run-'+run+'.nii.gz', \n",
    "                                             mask=imgroot+'second_level_analyses/MISC/individual_roi_masks/'+roi+'_indiv_roi_mask_run'+run+'.nii',\n",
    "                                             out_file=imgroot+'second_level_analyses/MISC/mean_roi_Temporal_Signal/Mean_temporal_signal_'+roi+'_'+task+'_run-'+run+'.txt')\n",
    "\n",
    "                    meants.cmdline\n",
    "                    meants.run()\n",
    "\n",
    "                    # save the mean signal as a list for each roi\n",
    "                    temporal_arrays[roicount] = txt2list(imgroot+'second_level_analyses/MISC/mean_roi_Temporal_Signal/Mean_temporal_signal_'+roi+'_'+task+'_run-'+run+'.txt', test)\n",
    "\n",
    "                    # identifying values for temporal var computation\n",
    "                    for item in range(0, len(final_exp_points)):\n",
    "                        templist = []\n",
    "                        for points in range (0, DISCRETE_POINTS):\n",
    "                            templist.append(temporal_arrays[roicount][(final_exp_points[item][points])-1]) # accounting for 0 index\n",
    "                        final_exp_values[item] = templist         # BOLD values of the temporal indices\n",
    "                        std_exp_values[item] = np.std(templist)   # std of each trial\n",
    "                        std_mean_values[item] = np.mean(std_exp_values[item]) # mean of all stds\n",
    "\n",
    "                    # save variance per roi\n",
    "                    TEMPORAL_VAR.append(std_mean_values)\n",
    "\n",
    "                ########################## INTERREGIONAL CORRELATION #########################################################\n",
    "                # make a correlation matrix for all rois' mean signal arrays\n",
    "                COR_MATRIX = np.corrcoef(temporal_arrays)                   \n",
    "\n",
    "                # save this per subject per run\n",
    "                np.save(imgroot+'second_level_analyses/interregional_cor/InterregionCor_R_'+task+'_run-'+run,COR_MATRIX)\n",
    "\n",
    "                # need to z-transform the R correlations \n",
    "                ZCorrMatrix = pearson2fisher(COR_MATRIX)\n",
    "                np.save(imgroot+'second_level_analyses/interregional_cor/InterregionCor_Z_'+task+'_run-'+run,ZCorrMatrix)  \n",
    "\n",
    "                # use a mask to convert diagonal elements to nans\n",
    "                mask = np.ones(ZCorrMatrix.shape, dtype=bool)\n",
    "                np.fill_diagonal(ZCorrMatrix, 'nan')\n",
    "\n",
    "                # calculate mean ignoring nans\n",
    "                Ztrans_values.append(ZCorrMatrix[~np.isnan(ZCorrMatrix)].mean())\n",
    "\n",
    "                pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "                ############ CREATING AND SAVING DATAFRAMES #############\n",
    "\n",
    "                # Create dataframe for temporal variance \n",
    "                dataframe_meas = [Ztrans_values, ['Z-transformed score of the mean pearsons R of correlation matrix relating each ROIs activation (excludes diagonal)']]\n",
    "                dataframe_MEAS = np.concatenate(dataframe_meas).ravel()\n",
    "                dataframe_COL = ['INTERREGION_COR', 'Description']\n",
    "                SUB_measures = pd.DataFrame([dataframe_MEAS],\n",
    "                                         columns=dataframe_COL)\n",
    "                SUB_measures.rename(index={0: 'Correlation_Z_transf'})\n",
    "\n",
    "                # Create dataframe for temporal variance \n",
    "                tempv_0 = [item[0] for item in TEMPORAL_VAR]\n",
    "                tempv_0.extend(['temporal variance within subject, within roi, for '+faces[0]+' condition'])\n",
    "                dataframe_meas1 = [tempv_0]\n",
    "                dataframe_MEA1 = np.concatenate(dataframe_meas1).ravel()\n",
    "                dataframe_col2 = ['TEMPVAR_FACES_' + s for s in roi_index]\n",
    "                dataframe_col2.extend(['Description'])\n",
    "                sub_measures2 = pd.DataFrame([dataframe_MEA1],\n",
    "                        columns=dataframe_col2)\n",
    "\n",
    "                # save dataframes as csv files\n",
    "                if iteration == 1:\n",
    "                    SUB_measures.to_csv(imgroot+'second_level_analyses/interregional_cor/'+subject+'_'+task+'_INTERREG_SUM_STATS.csv', mode= 'w', index=False)\n",
    "                    sub_measures2.to_csv(imgroot+'second_level_analyses/temporal_variance/'+subject+'_'+task+'_TEMPNOISE_SUM_STATS.csv', mode= 'w', index=False)\n",
    "                else:\n",
    "                    SUB_measures.to_csv(imgroot+'second_level_analyses/interregional_cor/'+subject+'_'+task+'_INTERREG_SUM_STATS.csv', mode= 'a', index=False, header=False)\n",
    "                    sub_measures2.to_csv(imgroot+'second_level_analyses/temporal_variance/'+subject+'_'+task+'_TEMPNOISE_SUM_STATS.csv', mode= 'a', index=False, header=False)\n",
    "                \n",
    "            # if run is never found in inclusion list, will not run analysis\n",
    "            else:\n",
    "                print(subject+task+run+' is a motion outlier')\n",
    "\n",
    "        file_list = [imgroot+'second_level_analyses/interregional_cor/'+subject+'_'+task+'_INTERREG_SUM_STATS.csv',\n",
    "                imgroot+'second_level_analyses/temporal_variance/'+subject+'_'+task+'_TEMPNOISE_SUM_STATS.csv'] \n",
    "\n",
    "        for file in file_list:\n",
    "            # now average all runs together\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                ex = pd.DataFrame(df.mean())\n",
    "                ex = ex.transpose()\n",
    "                ex.to_csv(file[:-4]+'_final.csv', mode='w', index=False)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Valid Runs for SAX_OA_098_tomloc\n",
      "No Valid Runs for SAX_OA_098_tomloc\n",
      "No Valid Runs for SAX_OA_009_tomloc\n",
      "No Valid Runs for SAX_OA_009_tomloc\n",
      "No Valid Runs for SAX_OA_005_tomloc\n",
      "No Valid Runs for SAX_OA_005_tomloc\n",
      "No Valid Runs for SAX_OA_032_tomloc\n",
      "No Valid Runs for SAX_OA_032_tomloc\n",
      "No Valid Runs for SAX_OA_012_tomloc\n",
      "No Valid Runs for SAX_OA_012_tomloc\n",
      "No Valid Runs for SAX_OA_004_tomloc\n",
      "No Valid Runs for SAX_OA_004_tomloc\n",
      "No Valid Runs for SAX_OA_081_tomloc\n",
      "No Valid Runs for SAX_OA_081_tomloc\n",
      "No Valid Runs for SAX_OA_049_tomloc\n",
      "No Valid Runs for SAX_OA_049_tomloc\n",
      "No Valid Runs for SAX_OA_016_tomloc\n",
      "No Valid Runs for SAX_OA_016_tomloc\n",
      "No Valid Runs for SAX_OA_101_tomloc\n",
      "No Valid Runs for SAX_OA_101_tomloc\n",
      "No Valid Runs for SAX_OA_096_tomloc\n",
      "No Valid Runs for SAX_OA_096_tomloc\n",
      "No Valid Runs for SAX_OA_096_tomloc\n",
      "No Valid Runs for SAX_OA_096_tomloc\n",
      "No Valid Runs for SAX_OA_108_tomloc\n",
      "No Valid Runs for SAX_OA_108_tomloc\n",
      "No Valid Runs for SAX_OA_083_tomloc\n",
      "No Valid Runs for SAX_OA_083_tomloc\n",
      "No Valid Runs for SAX_OA_036_tomloc\n",
      "No Valid Runs for SAX_OA_036_tomloc\n",
      "No Valid Runs for SAX_OA_038_tomloc\n",
      "No Valid Runs for SAX_OA_038_tomloc\n",
      "No Valid Runs for SAX_OA_109_tomloc\n",
      "No Valid Runs for SAX_OA_109_tomloc\n",
      "No Valid Runs for SAX_OA_111_tomloc\n",
      "No Valid Runs for SAX_OA_111_tomloc\n",
      "No Valid Runs for SAX_OA_011_tomloc\n",
      "No Valid Runs for SAX_OA_011_tomloc\n",
      "No Valid Runs for SAX_OA_041_tomloc\n",
      "No Valid Runs for SAX_OA_041_tomloc\n",
      "No Valid Runs for SAX_OA_040_tomloc\n",
      "No Valid Runs for SAX_OA_040_tomloc\n",
      "No Valid Runs for SAX_OA_044_tomloc\n",
      "No Valid Runs for SAX_OA_044_tomloc\n",
      "No Valid Runs for SAX_OA_044_tomloc\n",
      "No Valid Runs for SAX_OA_044_tomloc\n",
      "No Valid Runs for SAX_OA_099_tomloc\n",
      "No Valid Runs for SAX_OA_099_tomloc\n",
      "No Valid Runs for SAX_OA_054_tomloc\n",
      "No Valid Runs for SAX_OA_054_tomloc\n",
      "No Valid Runs for SAX_OA_075_tomloc\n",
      "No Valid Runs for SAX_OA_075_tomloc\n",
      "No Valid Runs for SAX_OA_082_tomloc\n",
      "No Valid Runs for SAX_OA_082_tomloc\n",
      "No Valid Runs for SAX_OA_034_tomloc\n",
      "No Valid Runs for SAX_OA_034_tomloc\n",
      "No Valid Runs for SAX_OA_034_tomloc\n",
      "No Valid Runs for SAX_OA_034_tomloc\n",
      "No Valid Runs for SAX_OA_051_tomloc\n",
      "No Valid Runs for SAX_OA_051_tomloc\n",
      "No Valid Runs for SAX_OA_028_tomloc\n",
      "No Valid Runs for SAX_OA_028_tomloc\n",
      "No Valid Runs for SAX_OA_048_tomloc\n",
      "No Valid Runs for SAX_OA_048_tomloc\n",
      "No Valid Runs for SAX_OA_027_tomloc\n",
      "No Valid Runs for SAX_OA_027_tomloc\n",
      "No Valid Runs for SAX_OA_061_tomloc\n",
      "No Valid Runs for SAX_OA_061_tomloc\n",
      "No Valid Runs for SAX_OA_026_tomloc\n",
      "No Valid Runs for SAX_OA_026_tomloc\n",
      "No Valid Runs for SAX_OA_015_tomloc\n",
      "No Valid Runs for SAX_OA_015_tomloc\n",
      "No Valid Runs for SAX_OA_015_tomloc\n",
      "No Valid Runs for SAX_OA_015_tomloc\n",
      "No Valid Runs for SAX_OA_037_tomloc\n",
      "No Valid Runs for SAX_OA_037_tomloc\n",
      "No Valid Runs for SAX_OA_078_tomloc\n",
      "No Valid Runs for SAX_OA_078_tomloc\n",
      "No Valid Runs for SAX_OA_042_tomloc\n",
      "No Valid Runs for SAX_OA_042_tomloc\n",
      "No Valid Runs for SAX_OA_056_tomloc\n",
      "No Valid Runs for SAX_OA_056_tomloc\n",
      "No Valid Runs for SAX_OA_022_tomloc\n",
      "No Valid Runs for SAX_OA_022_tomloc\n",
      "No Valid Runs for SAX_OA_094_tomloc\n",
      "No Valid Runs for SAX_OA_094_tomloc\n",
      "No Valid Runs for SAX_OA_033_tomloc\n",
      "No Valid Runs for SAX_OA_033_tomloc\n",
      "No Valid Runs for SAX_OA_033_tomloc\n",
      "No Valid Runs for SAX_OA_033_tomloc\n",
      "No Valid Runs for SAX_OA_091_tomloc\n",
      "No Valid Runs for SAX_OA_091_tomloc\n",
      "No Valid Runs for SAX_OA_091_tomloc\n",
      "No Valid Runs for SAX_OA_091_tomloc\n",
      "No Valid Runs for SAX_OA_085_tomloc\n",
      "No Valid Runs for SAX_OA_085_tomloc\n",
      "No Valid Runs for SAX_OA_047_tomloc\n",
      "No Valid Runs for SAX_OA_047_tomloc\n",
      "No Valid Runs for SAX_OA_055_tomloc\n",
      "No Valid Runs for SAX_OA_055_tomloc\n",
      "No Valid Runs for SAX_OA_002_tomloc\n",
      "No Valid Runs for SAX_OA_002_tomloc\n",
      "No Valid Runs for SAX_OA_080_tomloc\n",
      "No Valid Runs for SAX_OA_080_tomloc\n",
      "No Valid Runs for SAX_OA_080_tomloc\n",
      "No Valid Runs for SAX_OA_080_tomloc\n",
      "No Valid Runs for SAX_OA_030_tomloc\n",
      "No Valid Runs for SAX_OA_030_tomloc\n",
      "No Valid Runs for SAX_OA_064_tomloc\n",
      "No Valid Runs for SAX_OA_064_tomloc\n",
      "No Valid Runs for SAX_OA_064_tomloc\n",
      "No Valid Runs for SAX_OA_064_tomloc\n",
      "No Valid Runs for SAX_OA_029_tomloc\n",
      "No Valid Runs for SAX_OA_029_tomloc\n",
      "No Valid Runs for SAX_OA_025_tomloc\n",
      "No Valid Runs for SAX_OA_025_tomloc\n",
      "No Valid Runs for SAX_OA_097_tomloc\n",
      "No Valid Runs for SAX_OA_097_tomloc\n",
      "No Valid Runs for SAX_OA_104_tomloc\n",
      "No Valid Runs for SAX_OA_104_tomloc\n",
      "No Valid Runs for SAX_OA_104_tomloc\n",
      "No Valid Runs for SAX_OA_104_tomloc\n",
      "No Valid Runs for SAX_OA_018_tomloc\n",
      "No Valid Runs for SAX_OA_018_tomloc\n",
      "No Valid Runs for SAX_OA_084_tomloc\n",
      "No Valid Runs for SAX_OA_084_tomloc\n",
      "No Valid Runs for SAX_OA_100_tomloc\n",
      "No Valid Runs for SAX_OA_100_tomloc\n",
      "No Valid Runs for SAX_OA_100_tomloc\n",
      "No Valid Runs for SAX_OA_100_tomloc\n",
      "No Valid Runs for SAX_OA_050_tomloc\n",
      "No Valid Runs for SAX_OA_050_tomloc\n",
      "No Valid Runs for SAX_OA_031_tomloc\n",
      "No Valid Runs for SAX_OA_031_tomloc\n",
      "No Valid Runs for SAX_OA_057_tomloc\n",
      "No Valid Runs for SAX_OA_057_tomloc\n",
      "No Valid Runs for SAX_OA_095_tomloc\n",
      "No Valid Runs for SAX_OA_095_tomloc\n",
      "No Valid Runs for SAX_OA_076_tomloc\n",
      "No Valid Runs for SAX_OA_076_tomloc\n",
      "No Valid Runs for SAX_OA_058_tomloc\n",
      "No Valid Runs for SAX_OA_058_tomloc\n",
      "No Valid Runs for SAX_OA_010_tomloc\n",
      "No Valid Runs for SAX_OA_010_tomloc\n",
      "No Valid Runs for SAX_OA_013_tomloc\n",
      "No Valid Runs for SAX_OA_013_tomloc\n",
      "No Valid Runs for SAX_OA_052_tomloc\n",
      "No Valid Runs for SAX_OA_052_tomloc\n",
      "No Valid Runs for SAX_OA_106_tomloc\n",
      "No Valid Runs for SAX_OA_106_tomloc\n",
      "No Valid Runs for SAX_OA_106_tomloc\n",
      "No Valid Runs for SAX_OA_106_tomloc\n",
      "No Valid Runs for SAX_OA_066_tomloc\n",
      "No Valid Runs for SAX_OA_066_tomloc\n",
      "No Valid Runs for SAX_OA_020_tomloc\n",
      "No Valid Runs for SAX_OA_020_tomloc\n",
      "No Valid Runs for SAX_OA_017_tomloc\n",
      "No Valid Runs for SAX_OA_017_tomloc\n",
      "No Valid Runs for SAX_OA_039_tomloc\n",
      "No Valid Runs for SAX_OA_039_tomloc\n",
      "No Valid Runs for SAX_OA_105_tomloc\n",
      "No Valid Runs for SAX_OA_105_tomloc\n",
      "No Valid Runs for SAX_OA_063_tomloc\n",
      "No Valid Runs for SAX_OA_063_tomloc\n",
      "No Valid Runs for SAX_OA_053_tomloc\n",
      "No Valid Runs for SAX_OA_053_tomloc\n",
      "No Valid Runs for SAX_OA_102_tomloc\n",
      "No Valid Runs for SAX_OA_102_tomloc\n",
      "No Valid Runs for SAX_OA_046_tomloc\n",
      "No Valid Runs for SAX_OA_046_tomloc\n",
      "No Valid Runs for SAX_OA_103_tomloc\n",
      "No Valid Runs for SAX_OA_103_tomloc\n",
      "No Valid Runs for SAX_OA_086_tomloc\n",
      "No Valid Runs for SAX_OA_086_tomloc\n",
      "No Valid Runs for SAX_OA_035_tomloc\n",
      "No Valid Runs for SAX_OA_035_tomloc\n",
      "No Valid Runs for SAX_OA_077_tomloc\n",
      "No Valid Runs for SAX_OA_077_tomloc\n",
      "No Valid Runs for SAX_OA_093_tomloc\n",
      "No Valid Runs for SAX_OA_093_tomloc\n",
      "No Valid Runs for SAX_OA_065_tomloc\n",
      "No Valid Runs for SAX_OA_065_tomloc\n",
      "No Valid Runs for SAX_OA_024_tomloc\n",
      "No Valid Runs for SAX_OA_024_tomloc\n",
      "No Valid Runs for SAX_OA_023_tomloc\n",
      "No Valid Runs for SAX_OA_023_tomloc\n",
      "No Valid Runs for SAX_OA_043_tomloc\n",
      "No Valid Runs for SAX_OA_043_tomloc\n",
      "No Valid Runs for SAX_OA_079_tomloc\n",
      "No Valid Runs for SAX_OA_079_tomloc\n",
      "No Valid Runs for SAX_OA_079_tomloc\n",
      "No Valid Runs for SAX_OA_079_tomloc\n",
      "No Valid Runs for SAX_OA_110_tomloc\n",
      "No Valid Runs for SAX_OA_110_tomloc\n",
      "No Valid Runs for SAX_OA_062_tomloc\n",
      "No Valid Runs for SAX_OA_062_tomloc\n",
      "No Valid Runs for SAX_OA_062_tomloc\n",
      "No Valid Runs for SAX_OA_062_tomloc\n",
      "No Valid Runs for SAX_OA_087_tomloc\n",
      "No Valid Runs for SAX_OA_087_tomloc\n",
      "No Valid Runs for SAX_OA_087_tomloc\n",
      "No Valid Runs for SAX_OA_087_tomloc\n",
      "No Valid Runs for SAX_OA_060_tomloc\n",
      "No Valid Runs for SAX_OA_060_tomloc\n",
      "No Valid Runs for SAX_OA_021_tomloc\n",
      "No Valid Runs for SAX_OA_021_tomloc\n",
      "No Valid Runs for SAX_OA_088_tomloc\n",
      "No Valid Runs for SAX_OA_088_tomloc\n",
      "No Valid Runs for SAX_OA_059_tomloc\n",
      "No Valid Runs for SAX_OA_059_tomloc\n",
      "No Valid Runs for SAX_OA_007_tomloc\n",
      "No Valid Runs for SAX_OA_007_tomloc\n",
      "No Valid Runs for SAX_OA_089_tomloc\n",
      "No Valid Runs for SAX_OA_089_tomloc\n",
      "No Valid Runs for SAX_OA_089_tomloc\n",
      "No Valid Runs for SAX_OA_089_tomloc\n",
      "No Valid Runs for SAX_OA_019_tomloc\n",
      "No Valid Runs for SAX_OA_019_tomloc\n",
      "No Valid Runs for SAX_OA_107_tomloc\n",
      "No Valid Runs for SAX_OA_107_tomloc\n",
      "No Valid Runs for SAX_OA_092_tomloc\n",
      "No Valid Runs for SAX_OA_092_tomloc\n",
      "No Valid Runs for SAX_OA_071_tomloc\n",
      "No Valid Runs for SAX_OA_071_tomloc\n"
     ]
    }
   ],
   "source": [
    "# now put all data in one big matrix (subject--task)\n",
    "count = -1\n",
    "\n",
    "tasks = ['tomloc'] # just for tomloc\n",
    "count = 0\n",
    "counter = 0\n",
    "\n",
    "for subject in subject_list:\n",
    "    \n",
    "    # add task\n",
    "    for task in tasks:\n",
    "    \n",
    "        dfheader = []\n",
    "        counter = counter + 1\n",
    "        imgroot = main_root+'SUBJECTS/'+subject+'/standard/second_level_analyses/'\n",
    "        metric_folders = ['magnitude' ,'lateralization','interregional_cor','temporal_variance']\n",
    "        df0 = pd.DataFrame([[subject, task]],\n",
    "                                  columns=['SAX_ID', 'TASK'])\n",
    "        dflist = [df0]\n",
    "\n",
    "        for metric in metric_folders:\n",
    "            file = ''\n",
    "            if metric is 'magnitude':\n",
    "                file = [subject+'_'+task+'_'+'MAG_SUM_STATS_AVE_RUNS.csv']\n",
    "            elif metric is 'lateralization':\n",
    "                file = [subject+'_'+task+'_'+'LAT_SUM_STATS_AVE_RUNS.csv']\n",
    "            elif metric is 'interregional_cor':\n",
    "                file = [subject+'_'+task+'_'+'INTERREG_SUM_STATS_final.csv']\n",
    "            elif metric is 'temporal_variance':\n",
    "                file = [subject+'_'+task+'_'+'TEMPNOISE_SUM_STATS_final.csv']\n",
    "\n",
    "            for ext in range(0,len(file)):\n",
    "                newimgroot = imgroot + metric + '/' + file[ext]\n",
    "\n",
    "                try:\n",
    "                    df = pd.read_csv(newimgroot)\n",
    "                    dflist.append(df[:1])  # add the last row to the list\n",
    "                    #print(df[:1])\n",
    "                    if count <= 1:\n",
    "                        dfheader.append(df.columns.values.tolist())\n",
    "                except FileNotFoundError:\n",
    "                    print(\"No Valid Runs for \"+subject+'_'+task)   \n",
    "        # concatenate list of rows (all dataframes) for one subject\n",
    "        df_concat = pd.concat(dflist, axis=1)\n",
    "        count = count + 1\n",
    "\n",
    "\n",
    "        if counter == 1:\n",
    "            header = [item for sublist in dfheader for item in sublist]\n",
    "            add2header = ['SAX_OA_ID','TASK']\n",
    "            header = add2header+header\n",
    "            df_concat.to_csv(main_root+'/SUMMARY_STATS/ALLSUBJECTS_MATRIX.csv', index=False, mode='w', header=header)\n",
    "        else:\n",
    "            df_concat.to_csv(main_root+'/SUMMARY_STATS/ALLSUBJECTS_MATRIX.csv', index=False, mode='a', header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
